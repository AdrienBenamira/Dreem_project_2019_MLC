{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for common spatial pattern (CSP) and Riemannian method feature calculation and classification for EEG data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from tools.csp import generate_projection, generate_eye, extract_feature\n",
    "from tools.filters import load_filterbank\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tools.data import DreemDatasets\n",
    "from preprocessing import Compose, ExtractBands, ExtractSpectrum\n",
    "from models.riemannian_multiscale import riemannian_multiscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 50.  # sampling frequency\n",
    "NO_channels = 7  # number of EEG channels\n",
    "NO_riem = int(NO_channels * NO_channels + 1) / 2  # Total number of CSP feature per band and timewindow\n",
    "bw = np.array([2, 4, 8, 13, 22])\n",
    "ftype = 'butter'  # 'fir', 'butter'\n",
    "forder = 2  # 4\n",
    "filter_bank = load_filterbank(bw, fs, order=forder, max_freq=23, ftype=ftype)  # get filterbank coeffs\n",
    "time_windows_flt = np.array([[0, 30],\n",
    "                            [5, 25],\n",
    "                             [15, 30],\n",
    "                             [10, 25],\n",
    "                             [5, 20],\n",
    "                             [5, 15],\n",
    "                             [0, 10],\n",
    "                             [5, 15],\n",
    "                             [15, 25],\n",
    "                             [10, 20],\n",
    "                             [5, 15],\n",
    "                             [5, 10]]) * fs\n",
    "time_windows = time_windows_flt.astype(int)\n",
    "# restrict time windows and frequency bands\n",
    "#time_windows = time_windows[0:1]  # use only largest timewindow\n",
    "\n",
    "NO_bands = filter_bank.shape[0]\n",
    "riem_opt = \"No_Adaptation\"  # {\"Riemann\",\"Riemann_Euclid\",\"Whitened_Euclid\",\"No_Adaptation\"}\n",
    "rho = 0.1\n",
    "NO_csp = 20  # Total number of CSP feature per band and timewindow\n",
    "useCSP = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset qui vont biens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut : \n",
    "    \n",
    "    1- Tout le train sans split\n",
    "    2- Tout le train avec split\n",
    "    3- Tout le train équilibré sans split\n",
    "    4- Tout le train équilibré avec split\n",
    "    5- Tout le test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_set, val_set = DreemDatasets(\\'dataset/train.h5\\', \\'dataset/train_y.csv\\', \\n                                   split_train_val=1, seed=seed, balance_data=False,keep_datasets=use_datasets).get()\\ntrain_set.save_data(\"dataset/all/train\")\\ntrain_set.close()  # Ne ferme que les fichiers h5. Si mis en mémoire, on a toujours accès aux données !\\nval_set.close()\\ntrain_set, val_set = DreemDatasets(\\'dataset/train.h5\\', \\'dataset/train_y.csv\\', \\n                                   split_train_val=0.8, seed=seed, balance_data=False,keep_datasets=use_datasets).get()\\ntrain_set.save_data(\"dataset/all/train_split\")\\nval_set.save_data(\"dataset/all/val_split\")\\ntrain_set.close()  # Ne ferme que les fichiers h5. Si mis en mémoire, on a toujours accès aux données !\\nval_set.close()\\ntrain_set, val_set = DreemDatasets(\\'dataset/train.h5\\', \\'dataset/train_y.csv\\', \\n                                   split_train_val=1, seed=seed, balance_data=True,keep_datasets=use_datasets).get()\\ntrain_set.save_data(\"dataset/balanced/train\")\\ntrain_set.close()  # Ne ferme que les fichiers h5. Si mis en mémoire, on a toujours accès aux données !\\nval_set.close()\\ntrain_set, val_set = DreemDatasets(\\'dataset/train.h5\\', \\'dataset/train_y.csv\\', \\n                                   split_train_val=0.8, seed=seed, balance_data=True,keep_datasets=use_datasets).get()\\ntrain_set.save_data(\"dataset/balanced/train_split\")\\nval_set.save_data(\"dataset/balanced/val_split\")\\ntrain_set.close()  # Ne ferme que les fichiers h5. Si mis en mémoire, on a toujours accès aux données !\\nval_set.close()\\n\\n\\nfrom tools.data import DreemDataset\\ntest_set = DreemDataset(\\'dataset/test.h5\\', keep_datasets=use_datasets).init()\\ntest_set.save_data(\"dataset/all/test\")\\ntest_set.close()'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_datasets = ['accelerometer_x','accelerometer_y','accelerometer_z','eeg_1', 'eeg_2', 'eeg_3', 'eeg_4', 'eeg_5', 'eeg_6', 'eeg_7', 'pulse_oximeter_infrared']\n",
    "seed = 1\n",
    "\"\"\"train_set, val_set = DreemDatasets('dataset/train.h5', 'dataset/train_y.csv', \n",
    "                                   split_train_val=1, seed=seed, balance_data=False,keep_datasets=use_datasets).get()\n",
    "train_set.save_data(\"dataset/all/train\")\n",
    "train_set.close()  # Ne ferme que les fichiers h5. Si mis en mémoire, on a toujours accès aux données !\n",
    "val_set.close()\n",
    "train_set, val_set = DreemDatasets('dataset/train.h5', 'dataset/train_y.csv', \n",
    "                                   split_train_val=0.8, seed=seed, balance_data=False,keep_datasets=use_datasets).get()\n",
    "train_set.save_data(\"dataset/all/train_split\")\n",
    "val_set.save_data(\"dataset/all/val_split\")\n",
    "train_set.close()  # Ne ferme que les fichiers h5. Si mis en mémoire, on a toujours accès aux données !\n",
    "val_set.close()\n",
    "train_set, val_set = DreemDatasets('dataset/train.h5', 'dataset/train_y.csv', \n",
    "                                   split_train_val=1, seed=seed, balance_data=True,keep_datasets=use_datasets).get()\n",
    "train_set.save_data(\"dataset/balanced/train\")\n",
    "train_set.close()  # Ne ferme que les fichiers h5. Si mis en mémoire, on a toujours accès aux données !\n",
    "val_set.close()\n",
    "train_set, val_set = DreemDatasets('dataset/train.h5', 'dataset/train_y.csv', \n",
    "                                   split_train_val=0.8, seed=seed, balance_data=True,keep_datasets=use_datasets).get()\n",
    "train_set.save_data(\"dataset/balanced/train_split\")\n",
    "val_set.save_data(\"dataset/balanced/val_split\")\n",
    "train_set.close()  # Ne ferme que les fichiers h5. Si mis en mémoire, on a toujours accès aux données !\n",
    "val_set.close()\n",
    "\n",
    "\n",
    "from tools.data import DreemDataset\n",
    "test_set = DreemDataset('dataset/test.h5', keep_datasets=use_datasets).init()\n",
    "test_set.save_data(\"dataset/all/test\")\n",
    "test_set.close()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6765, 7, 1500) (6765,)\n"
     ]
    }
   ],
   "source": [
    "def get_data(path, train= True,  one_vs_all = False, limit= None):\n",
    "    if train:\n",
    "        for i in range(7):\n",
    "            if i==0:\n",
    "                feature_0 = np.load(\"dataset/\"+path+\"/train/eeg_\" + str(i + 1) + \".npy\")\n",
    "                X = np.zeros((7, feature_0.shape[0], feature_0.shape[1]))\n",
    "                X[0] = feature_0\n",
    "                del feature_0\n",
    "            else:\n",
    "                X[i] = np.load(\"dataset/\"+path+\"/train/eeg_\" + str(i + 1) + \".npy\")\n",
    "        Y = np.load(\"dataset/\"+path+\"/train/targets.npy\")\n",
    "        X = X.transpose((1, 0, 2))\n",
    "    else:\n",
    "        for i in range(7):\n",
    "            if i==0:\n",
    "                feature_0 = np.load(\"dataset/\"+path+\"/val_split/eeg_\" + str(i + 1) + \".npy\")\n",
    "                X = np.zeros((7, feature_0.shape[0], feature_0.shape[1]))\n",
    "                X[0] = feature_0\n",
    "                del feature_0\n",
    "            else:\n",
    "                X[i] = np.load(\"dataset/\"+path+\"/val_split/eeg_\" + str(i + 1) + \".npy\")\n",
    "        Y = np.load(\"dataset/\"+path+\"/val_split/targets.npy\")\n",
    "        X = X.transpose((1, 0, 2))\n",
    "    if one_vs_all:\n",
    "        Y[Y > 2] = 0\n",
    "        Y[Y < 2] = 0\n",
    "        Y[Y == 2] = 1\n",
    "    if limit is not None:\n",
    "        X = X[:limit]\n",
    "        Y = Y[:limit]\n",
    "    return(X, Y)\n",
    "\n",
    "\n",
    "\n",
    "path = \"balanced\"\n",
    "train_data, train_label = get_data(path, train = True, one_vs_all = False)\n",
    "path = \"all\"\n",
    "eval_data, eval_label = get_data(path, train = False, one_vs_all = False)\n",
    "print(train_data.shape, train_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction des features par CSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(data, label, time_windows, useCSP = True, NO_csp = 20):\n",
    "    if useCSP:\n",
    "        w = generate_projection(data, label, NO_csp, filter_bank, time_windows, NO_classes=5)\n",
    "    else:\n",
    "        w = generate_eye(data, label, filter_bank, time_windows)\n",
    "    feature_mat = extract_feature(data, w, filter_bank, time_windows)\n",
    "    return(w, feature_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, train_feat_CSP = get_features(train_data, train_label, time_windows, useCSP)\n",
    "eval_feature_CSP = extract_feature(eval_data, w, filter_bank, time_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction des features par Riemann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "riemann = riemannian_multiscale(filter_bank, time_windows, riem_opt=riem_opt, rho=rho, vectorized=True)\n",
    "train_feat_R = riemann.fit(train_data)\n",
    "eval_feature_R = riemann.features(eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_CSP = RandomForestClassifier(n_estimators=500, random_state=0)\n",
    "RF_CSP.fit(train_feat_CSP, train_label)\n",
    "\n",
    "RF_R = RandomForestClassifier(n_estimators=500, random_state=0)\n",
    "RF_R.fit(train_feat_R, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 627   62   21    8   17]\n",
      " [   0  260    0    0    0]\n",
      " [ 177  323 1864  546  527]\n",
      " [  28   37   85 1021   15]\n",
      " [ 115  222  256   83 1364]] 0.6706711935231131 0.6513909892605189\n",
      "[[ 619   79   16    3   18]\n",
      " [   0  260    0    0    0]\n",
      " [ 146  363 1832  545  551]\n",
      " [  31   33   98 1014   10]\n",
      " [ 117  231  248   77 1367]] 0.6649255680334291 0.6459635244279128\n"
     ]
    }
   ],
   "source": [
    "labels_pred = RF_CSP.predict(eval_feature_CSP)\n",
    "CM = confusion_matrix(eval_label, labels_pred)\n",
    "Acc = accuracy_score(eval_label, labels_pred)\n",
    "F1 = f1_score(eval_label, labels_pred, average='macro')\n",
    "\n",
    "print(CM, Acc, F1)\n",
    "\n",
    "labels_pred = RF_R.predict(eval_feature_R)\n",
    "CM = confusion_matrix(eval_label, labels_pred)\n",
    "Acc = accuracy_score(eval_label, labels_pred)\n",
    "F1 = f1_score(eval_label, labels_pred, average='macro')\n",
    "\n",
    "print(CM, Acc, F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=\"balanced\"\n",
    "train_extra_data, train_extra_label = get_extra_data(path, train = True, one_vs_all = False)\n",
    "path=\"all\"\n",
    "eval_extra_data, eval_extra_label = get_extra_data(path, train = False, one_vs_all = False)\n",
    "X = train_extra_data.reshape(-1, 4*16)\n",
    "X_val = eval_extra_data.reshape(-1,  4*16)\n",
    "\n",
    "all_final_features = np.concatenate((train_feat_CSP, X), axis= 1)\n",
    "all_final_features_R = np.concatenate((train_feat_R, X), axis= 1)\n",
    "all_final_features_val = np.concatenate((eval_feature_CSP, X_val), axis= 1)\n",
    "all_final_features_val_R = np.concatenate((eval_feature_R, X_val), axis= 1)\n",
    "\n",
    "\n",
    "RF_CSP = RandomForestClassifier(n_estimators=500, random_state=0)\n",
    "RF_CSP.fit(all_final_features, train_label)\n",
    "\n",
    "RF_R = RandomForestClassifier(n_estimators=500, random_state=0)\n",
    "RF_R.fit(all_final_features_R, train_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 630   69   18    4   14]\n",
      " [   0  260    0    0    0]\n",
      " [ 163  299 1904  540  531]\n",
      " [  25   35   86 1024   16]\n",
      " [ 108  187  261   93 1391]] 0.6802037085400888 0.6621325628028887\n",
      "[[ 613   85   13    4   20]\n",
      " [   0  260    0    0    0]\n",
      " [ 132  358 1861  544  542]\n",
      " [  32   37   99 1004   14]\n",
      " [ 107  210  250   84 1389]] 0.6694959519456777 0.6499583710353787\n"
     ]
    }
   ],
   "source": [
    "labels_pred = RF_CSP.predict(all_final_features_val)\n",
    "CM = confusion_matrix(eval_label, labels_pred)\n",
    "Acc = accuracy_score(eval_label, labels_pred)\n",
    "F1 = f1_score(eval_label, labels_pred, average='macro')\n",
    "\n",
    "print(CM, Acc, F1)\n",
    "\n",
    "labels_pred = RF_R.predict(all_final_features_val_R)\n",
    "CM = confusion_matrix(eval_label, labels_pred)\n",
    "Acc = accuracy_score(eval_label, labels_pred)\n",
    "F1 = f1_score(eval_label, labels_pred, average='macro')\n",
    "\n",
    "print(CM, Acc, F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faudrait : \n",
    "\n",
    "    1 - Comprendre pk N = 20 marche\n",
    "    2 - Faire des stats sur les méthodes (temps et accuracy)      ok\n",
    "    4 - Ajouter les features des 3 autres courbes + les probas    ok\n",
    "    \n",
    "    6 - Images + Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30631, 7, 1500) (30631,)\n"
     ]
    }
   ],
   "source": [
    "path = \"all\"\n",
    "train_data, train_label = get_data(path, train = True, one_vs_all = False)\n",
    "eval_data, eval_label = get_data(path, train = False, one_vs_all = False)\n",
    "print(train_data.shape, train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6050.65074467659\n",
      "[[ 502    5  165    4   59]\n",
      " [  37   13  149    0   61]\n",
      " [  53    4 2995   79  306]\n",
      " [  15    0  425  722   24]\n",
      " [  59    6  546    9 1420]] 0.7380517106294071 0.6062919675553691\n",
      "2918.735505580902\n",
      "[[ 464    8  179    4   80]\n",
      " [  40   16  133    0   71]\n",
      " [  76    7 2902  107  345]\n",
      " [  13    0  502  659   12]\n",
      " [  53    6  622   17 1342]] 0.7029250457038391 0.5768417304725457\n"
     ]
    }
   ],
   "source": [
    "useCSP = True\n",
    "NO_time_windows = int(time_windows.size / 2)\n",
    "start = time.time()\n",
    "w, train_feat_CSP = get_features(train_data, train_label, useCSP)\n",
    "RF_CSP = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "RF_CSP.fit(train_feat_CSP, train_label)\n",
    "eval_feature_CSP = extract_feature(eval_data, w, filter_bank, time_windows)\n",
    "np.save(\"features_CSP_train_split_True.npy\", train_feat_CSP)\n",
    "np.save(\"features_CSP_val_split_True.npy\", eval_feature_CSP)\n",
    "labels_pred = RF_CSP.predict(eval_feature_CSP)\n",
    "CM = confusion_matrix(eval_label, labels_pred)\n",
    "Acc = accuracy_score(eval_label, labels_pred)\n",
    "F1 = f1_score(eval_label, labels_pred, average='macro')\n",
    "print(time.time()-start)\n",
    "print(CM, Acc, F1)\n",
    "\n",
    "useCSP = False\n",
    "NO_time_windows = int(time_windows.size / 2)\n",
    "start = time.time()\n",
    "w, train_feat_CSP = get_features(train_data, train_label, useCSP)\n",
    "RF_CSP = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "RF_CSP.fit(train_feat_CSP, train_label)\n",
    "eval_feature_CSP = extract_feature(eval_data, w, filter_bank, time_windows)\n",
    "np.save(\"features_CSP_train_split_False.npy\", train_feat_CSP)\n",
    "np.save(\"features_CSP_val_split_False.npy\", eval_feature_CSP)\n",
    "labels_pred = RF_CSP.predict(eval_feature_CSP)\n",
    "CM = confusion_matrix(eval_label, labels_pred)\n",
    "Acc = accuracy_score(eval_label, labels_pred)\n",
    "F1 = f1_score(eval_label, labels_pred, average='macro')\n",
    "print(time.time()-start)\n",
    "print(CM, Acc, F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5549.880460977554\n",
      "[[ 480    5  179    0   71]\n",
      " [  44   12  140    0   64]\n",
      " [  55    6 2934  110  332]\n",
      " [  17    1  499  657   12]\n",
      " [  46    8  598   18 1370]] 0.7120658135283364 0.5808282267416092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pyriemann/utils/base.py:12: RuntimeWarning: invalid value encountered in log\n",
      "  eigvals = numpy.diag(operator(eigvals))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrices must be positive definite. Add regularization to avoid this error.\n",
      "Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "4730.435000419617\n",
      "[[ 480    2  196    5   52]\n",
      " [  48    6  159    3   44]\n",
      " [  52    3 3021   79  282]\n",
      " [  11    0  541  567   67]\n",
      " [  57    3  745   38 1197]] 0.6882998171846435 0.5475961588532392\n",
      "5817.639447450638\n",
      "[[ 480    5  179    0   71]\n",
      " [  44   12  140    0   64]\n",
      " [  55    6 2934  110  332]\n",
      " [  17    1  499  657   12]\n",
      " [  46    8  598   18 1370]] 0.7120658135283364 0.5808282267416092\n"
     ]
    }
   ],
   "source": [
    "methods = [\"No_Adaptation\", \"Riemann\",\"Riemann_Euclid\",\"Whitened_Euclid\"]\n",
    "NO_time_windows = time_windows.shape[0]\n",
    "for riem_opt in methods:\n",
    "    try:\n",
    "        start = time.time()\n",
    "        NO_time_windows = time_windows.shape[0]\n",
    "        NO_features = NO_riem * NO_bands * NO_time_windows\n",
    "        riemann = riemannian_multiscale(filter_bank, time_windows, riem_opt=riem_opt, rho=rho, vectorized=True)\n",
    "        train_feat_R = riemann.fit(train_data)\n",
    "        RF_R = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "        RF_R.fit(train_feat_R, train_label)\n",
    "        eval_feature_R = riemann.features(eval_data)\n",
    "        np.save(\"features_R_train_split_\"+str(riem_opt), train_feat_CSP)\n",
    "        np.save(\"features_R_val_split_\"+str(riem_opt), eval_feature_CSP)\n",
    "        labels_pred = RF_R.predict(eval_feature_R)\n",
    "        CM = confusion_matrix(eval_label, labels_pred)\n",
    "        Acc = accuracy_score(eval_label, labels_pred)\n",
    "        F1 = f1_score(eval_label, labels_pred, average='macro')\n",
    "        print(time.time()-start)\n",
    "        print(CM, Acc, F1)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF - Boosting - NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add features of other signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving into dataset/all/extra_eeg/test ...\n",
      "Loading dataset eeg_1 ...\n",
      "Apply transformations...\n",
      "Applied.\n",
      "Loading dataset eeg_2 ...\n",
      "Apply transformations...\n",
      "Applied.\n",
      "Loading dataset eeg_3 ...\n",
      "Apply transformations...\n",
      "Applied.\n",
      "Loading dataset eeg_4 ...\n",
      "Apply transformations...\n",
      "Applied.\n",
      "Loading dataset eeg_5 ...\n",
      "Apply transformations...\n",
      "Applied.\n",
      "Loading dataset eeg_6 ...\n",
      "Apply transformations...\n",
      "Applied.\n",
      "Loading dataset eeg_7 ...\n",
      "Apply transformations...\n",
      "Applied.\n",
      "Loading dataset accelerometer_x ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Not a dataset (not a dataset)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-701fc0d4eeb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDreemDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDreemDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset/test.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_datasets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_datasets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset/all/extra_eeg/test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/benamira/19793564030D4273/MCsBackup/3A/OMA/MLC/dreem-project/tools/data.py\u001b[0m in \u001b[0;36msave_data\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys_to_keep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh5_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Force not loading from path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/benamira/19793564030D4273/MCsBackup/3A/OMA/MLC/dreem-project/tools/data.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(self, dataset_name, path)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh5_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys_to_keep\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Only keep the keys_to_keep elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lazy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \"\"\"\n\u001b[1;32m    411\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mis_empty_dataspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mEllipsis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty datasets cannot be sliced\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/base.py\u001b[0m in \u001b[0;36mis_empty_dataspace\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_empty_dataspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;34m\"\"\" Check if an object's dataspace is empty \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_simple_extent_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNULL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.get_space\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Not a dataset (not a dataset)"
     ]
    }
   ],
   "source": [
    "from preprocessing.features import ExtractFeatures\n",
    "extract_features = ExtractFeatures(bands='*', features=['min', 'max', 'energy', 'frequency','mmd','esis'])\n",
    "use_datasets = [\"eeg_1\",\"eeg_2\",\"eeg_3\",\"eeg_4\",\"eeg_5\",\"eeg_6\",\"eeg_7\"]\n",
    "\n",
    "transformations = {\n",
    "    \"eeg_1\": extract_features,\n",
    "    \"eeg_2\": extract_features,\n",
    "    \"eeg_3\": extract_features,\n",
    "    \"eeg_4\":extract_features,\n",
    "    \"eeg_5\":extract_features,\n",
    "    \"eeg_6\":extract_features,\n",
    "    \"eeg_7\":extract_features\n",
    "}\n",
    "\n",
    "seed = 1\n",
    "\"\"\"\n",
    "train_set, val_set = DreemDatasets('dataset/train.h5', 'dataset/train_y.csv', \n",
    "                                   split_train_val=1, seed=seed, balance_data=False,keep_datasets=use_datasets,transforms=transformations).get()\n",
    "train_set.save_data(\"dataset/all/extra_eeg/train\")\n",
    "train_set.close()  # Ne ferme que les fichiers h5. Si mis en mémoire, on a toujours accès aux données !\n",
    "val_set.close()\n",
    "train_set, val_set = DreemDatasets('dataset/train.h5', 'dataset/train_y.csv', \n",
    "                                   split_train_val=0.8, seed=seed, balance_data=False,keep_datasets=use_datasets,transforms=transformations).get()\n",
    "train_set.save_data(\"dataset/all/extra_eeg/train_split\")\n",
    "val_set.save_data(\"dataset/all/extra_eeg/val_split\")\n",
    "train_set.close()  # Ne ferme que les fichiers h5. Si mis en mémoire, on a toujours accès aux données !\n",
    "val_set.close()\n",
    "train_set, val_set = DreemDatasets('dataset/train.h5', 'dataset/train_y.csv', \n",
    "                                   split_train_val=1, seed=seed, balance_data=True,keep_datasets=use_datasets,transforms=transformations).get()\n",
    "train_set.save_data(\"dataset/balanced/extra_eeg/train\")\n",
    "train_set.close()  # Ne ferme que les fichiers h5. Si mis en mémoire, on a toujours accès aux données !\n",
    "val_set.close()\n",
    "train_set, val_set = DreemDatasets('dataset/train.h5', 'dataset/train_y.csv', \n",
    "                                   split_train_val=0.8, seed=seed, balance_data=True,keep_datasets=use_datasets,transforms=transformations).get()\n",
    "train_set.save_data(\"dataset/balanced/extra_eeg/train_split\")\n",
    "val_set.save_data(\"dataset/balanced/extra_eeg/val_split\")\n",
    "train_set.close()  # Ne ferme que les fichiers h5. Si mis en mémoire, on a toujours accès aux données !\n",
    "val_set.close\n",
    "\"\"\"\n",
    "#from preprocessing.features import ExtractFeatures\n",
    "#extract_features = ExtractFeatures(bands='*', features=['min', 'max', 'energy', 'frequency'])\n",
    "#use_datasets = [\"eeg_1\",\"eeg_2\",\"eeg_3\",\"eeg_4\",\"eeg_5\",\"eeg_6\",\"eeg_7\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tools.data import DreemDataset\n",
    "test_set = DreemDataset('dataset/test.h5', keep_datasets=use_datasets,transforms=transformations).init()\n",
    "test_set.save_data(\"dataset/all/extra_eeg/test\")\n",
    "test_set.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extra_data(path, train= True,  one_vs_all = False, limit= None):\n",
    "    use_datasets = [\"accelerometer_x\",\"accelerometer_y\",\"accelerometer_z\",\"pulse_oximeter_infrared\"]\n",
    "    if train:\n",
    "        for i in range(4):\n",
    "            if i==0:\n",
    "                feature_0 = np.load(\"dataset/\"+path+\"/train/\" + use_datasets[i] + \".npy\").transpose((1, 0, 2))\n",
    "                X = np.zeros((4, feature_0.shape[0], feature_0.shape[1]*feature_0.shape[2]))\n",
    "                X[i] = feature_0.reshape(feature_0.shape[0], feature_0.shape[1]*feature_0.shape[2])\n",
    "                del feature_0\n",
    "            else:\n",
    "                feature_0 = np.load(\"dataset/\"+path+\"/train/\" + use_datasets[i] + \".npy\").transpose((1, 0, 2))\n",
    "                X[i] = feature_0.reshape(feature_0.shape[0], feature_0.shape[1]*feature_0.shape[2])\n",
    "                del feature_0\n",
    "        Y = np.load(\"dataset/\"+path+\"/train/targets.npy\")\n",
    "        X = X.transpose((1, 0, 2))\n",
    "    else:\n",
    "        for i in range(4):\n",
    "            if i==0:\n",
    "                feature_0 = np.load(\"dataset/\"+path+\"/val_split/\" + use_datasets[i] + \".npy\").transpose((1, 0, 2))\n",
    "                X = np.zeros((4, feature_0.shape[0], feature_0.shape[1]*feature_0.shape[2]))\n",
    "                X[i] = feature_0.reshape(feature_0.shape[0], feature_0.shape[1]*feature_0.shape[2])\n",
    "            else:\n",
    "                feature_0 = np.load(\"dataset/\"+path+\"/val_split/\" + use_datasets[i] + \".npy\").transpose((1, 0, 2))\n",
    "                X[i] = feature_0.reshape(feature_0.shape[0], feature_0.shape[1]*feature_0.shape[2])\n",
    "                del feature_0\n",
    "        Y = np.load(\"dataset/\"+path+\"/val_split/targets.npy\")\n",
    "        X = X.transpose((1, 0, 2))\n",
    "    if one_vs_all:\n",
    "        Y[Y > 2] = 0\n",
    "        Y[Y < 2] = 0\n",
    "        Y[Y == 2] = 1\n",
    "    if limit is not None:\n",
    "        X = X[:limit]\n",
    "        Y = Y[:limit]\n",
    "    return(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 7, 1500) (1000,)\n",
      "(1000, 4, 16) (1000, 4, 16)\n"
     ]
    }
   ],
   "source": [
    "path = \"balanced\"\n",
    "train_data, train_label = get_data(path, train = True, one_vs_all = False, limit= 1000)\n",
    "eval_data, eval_label = get_data(path, train = False, one_vs_all = False, limit= 1000)\n",
    "print(train_data.shape, train_label.shape)\n",
    "train_extra_data, train_extra_label = get_extra_data(path, train = True, one_vs_all = False, limit= 1000)\n",
    "eval_extra_data, eval_extra_label = get_extra_data(path, train = False, one_vs_all = False, limit= 1000)\n",
    "print(train_extra_data.shape, train_extra_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF with only extra data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[109  40  11  14  27]\n",
      " [ 65  70  19  34  22]\n",
      " [ 14  23  63  51  43]\n",
      " [  4  14  27 128  23]\n",
      " [ 19  25  33  39  83]] 0.453 0.44619586527672295\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "X = train_extra_data.reshape(-1, 4*16)\n",
    "X_val = eval_extra_data.reshape(-1,  4*16)\n",
    "clf.fit(X, train_extra_label)\n",
    "labels_pred = clf.predict(X_val)\n",
    "cm = confusion_matrix(eval_extra_label, labels_pred)\n",
    "acc = accuracy_score(eval_extra_label, labels_pred)\n",
    "f1 = f1_score(eval_extra_label, labels_pred, average='macro')\n",
    "print(cm, acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30631, 4, 16) (30631, 4, 16)\n",
      "[[ 379    2  249    4  101]\n",
      " [  76    2  153    2   27]\n",
      " [ 103    2 3048   64  220]\n",
      " [  26    0  434  677   49]\n",
      " [  99    2  853   14 1072]] 0.6761556542178114 0.5203379071054419\n"
     ]
    }
   ],
   "source": [
    "path = \"all\"\n",
    "train_extra_data, train_extra_label = get_extra_data(path, train = True, one_vs_all = False)\n",
    "eval_extra_data, eval_extra_label = get_extra_data(path, train = False, one_vs_all = False)\n",
    "print(train_extra_data.shape, train_extra_data.shape)\n",
    "clf = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "X = train_extra_data.reshape(-1, 4*16)\n",
    "X_val = eval_extra_data.reshape(-1,  4*16)\n",
    "clf.fit(X, train_extra_label)\n",
    "labels_pred = clf.predict(X_val)\n",
    "cm = confusion_matrix(eval_extra_label, labels_pred)\n",
    "acc = accuracy_score(eval_extra_label, labels_pred)\n",
    "f1 = f1_score(eval_extra_label, labels_pred, average='macro')\n",
    "print(cm, acc, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and eval with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30631, 4384)\n"
     ]
    }
   ],
   "source": [
    "path = \"all\"\n",
    "train_extra_data, train_extra_label = get_extra_data(path, train = True, one_vs_all = False)\n",
    "eval_extra_data, eval_extra_label = get_extra_data(path, train = False, one_vs_all = False)\n",
    "X = train_extra_data.reshape(-1, 4*16)\n",
    "X_val = eval_extra_data.reshape(-1,  4*16)\n",
    "features_CSP_train = np.load(\"dataset/all/features_CSP_train_split_False.npy\")\n",
    "features_CSP_val = np.load(\"dataset/all/features_CSP_val_split_False.npy\")\n",
    "all_final_features = np.concatenate((features_CSP_train, X), axis= 1)\n",
    "all_final_features_val = np.concatenate((features_CSP_val, X_val), axis= 1)\n",
    "print(all_final_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 532    7  142    1   53]\n",
      " [  54   19  133    0   54]\n",
      " [  59    4 2992   85  297]\n",
      " [  17    0  410  741   18]\n",
      " [  57   10  520    9 1444]] 0.7479759728388613 0.6229231518045213\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "clf.fit(all_final_features, train_extra_label)\n",
    "labels_pred = clf.predict(all_final_features_val)\n",
    "cm = confusion_matrix(eval_extra_label, labels_pred)\n",
    "acc = accuracy_score(eval_extra_label, labels_pred)\n",
    "f1 = f1_score(eval_extra_label, labels_pred, average='macro')\n",
    "print(cm, acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30631, 1576)\n",
      "[[ 492    9  158    1   75]\n",
      " [  48   18  130    0   64]\n",
      " [  67    9 2923  103  335]\n",
      " [  18    0  469  685   14]\n",
      " [  50   10  607   15 1358]] 0.7150692086706711 0.5919485074191881\n"
     ]
    }
   ],
   "source": [
    "path = \"all\"\n",
    "train_extra_data, train_extra_label = get_extra_data(path, train = True, one_vs_all = False)\n",
    "eval_extra_data, eval_extra_label = get_extra_data(path, train = False, one_vs_all = False)\n",
    "X = train_extra_data.reshape(-1, 4*16)\n",
    "X_val = eval_extra_data.reshape(-1,  4*16)\n",
    "features_CSP_train = np.load(\"dataset/all/features_CSP_train_split_False.npy\")\n",
    "features_CSP_val = np.load(\"dataset/all/features_CSP_val_split_False.npy\")\n",
    "all_final_features = np.concatenate((features_CSP_train, X), axis= 1)\n",
    "all_final_features_val = np.concatenate((features_CSP_val, X_val), axis= 1)\n",
    "print(all_final_features.shape)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "clf.fit(all_final_features, train_extra_label)\n",
    "labels_pred = clf.predict(all_final_features_val)\n",
    "cm = confusion_matrix(eval_extra_label, labels_pred)\n",
    "acc = accuracy_score(eval_extra_label, labels_pred)\n",
    "f1 = f1_score(eval_extra_label, labels_pred, average='macro')\n",
    "print(cm, acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30631, 1576)\n"
     ]
    }
   ],
   "source": [
    "path = \"all\"\n",
    "train_extra_data, train_extra_label = get_extra_data(path, train = True, one_vs_all = False)\n",
    "eval_extra_data, eval_extra_label = get_extra_data(path, train = False, one_vs_all = False)\n",
    "X = train_extra_data.reshape(-1, 4*16)\n",
    "X_val = eval_extra_data.reshape(-1,  4*16)\n",
    "features_CSP_train = np.load(\"dataset/all/features_R_train_split_No_Adaptation.npy\")\n",
    "features_CSP_val = np.load(\"dataset/all/features_R_val_split_No_Adaptation.npy\")\n",
    "all_final_features = np.concatenate((features_CSP_train, X), axis= 1)\n",
    "all_final_features_val = np.concatenate((features_CSP_val, X_val), axis= 1)\n",
    "print(all_final_features.shape)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "clf.fit(all_final_features, train_extra_label)\n",
    "labels_pred = clf.predict(all_final_features_val)\n",
    "cm = confusion_matrix(eval_extra_label, labels_pred)\n",
    "acc = accuracy_score(eval_extra_label, labels_pred)\n",
    "f1 = f1_score(eval_extra_label, labels_pred, average='macro')\n",
    "print(cm, acc, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tester : en supprimant des EEGS, en ajoutant les extrafeatures, en ajouant les extar features des EEGS, en mettant la filetr bank, pour 1000 et 5000 et 500 estimateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_f(momo, jojo,compteur, path, train= True,  one_vs_all = False, limit= None):\n",
    "    if train:\n",
    "        for i in range(jojo):\n",
    "            print(i)\n",
    "            if i==momo:\n",
    "                feature_0 = np.load(\"dataset/\"+path+\"/train_split/eeg_\" + str(i + 1) + \".npy\")\n",
    "                X = np.zeros((compteur, feature_0.shape[0], feature_0.shape[1]))\n",
    "                X[0] = feature_0\n",
    "                del feature_0\n",
    "            else:\n",
    "                X[i] = np.load(\"dataset/\"+path+\"/train_split/eeg_\" + str(i + 1) + \".npy\")\n",
    "        Y = np.load(\"dataset/\"+path+\"/train_split/targets.npy\")\n",
    "        X = X.transpose((1, 0, 2))\n",
    "    else:\n",
    "        for i in range(momo,jojo):\n",
    "            if i==momo:\n",
    "                feature_0 = np.load(\"dataset/\"+path+\"/val_split/eeg_\" + str(i + 1) + \".npy\")\n",
    "                X = np.zeros((compteur, feature_0.shape[0], feature_0.shape[1]))\n",
    "                X[0] = feature_0\n",
    "                del feature_0\n",
    "            else:\n",
    "                X[i] = np.load(\"dataset/\"+path+\"/val_split/eeg_\" + str(i + 1) + \".npy\")\n",
    "        Y = np.load(\"dataset/\"+path+\"/val_split/targets.npy\")\n",
    "        X = X.transpose((1, 0, 2))\n",
    "    if one_vs_all:\n",
    "        Y[Y > 2] = 0\n",
    "        Y[Y < 2] = 0\n",
    "        Y[Y == 2] = 1\n",
    "    if limit is not None:\n",
    "        X = X[:limit]\n",
    "        Y = Y[:limit]\n",
    "    return(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n",
      "local variable 'X' referenced before assignment\n",
      "2 0\n",
      "local variable 'X' referenced before assignment\n",
      "2 1\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "3 0\n",
      "local variable 'X' referenced before assignment\n",
      "3 1\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "3 2\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "4 0\n",
      "local variable 'X' referenced before assignment\n",
      "4 1\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "4 2\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "4 3\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "5 0\n",
      "local variable 'X' referenced before assignment\n",
      "5 1\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "5 2\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "5 3\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "5 4\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "6 0\n",
      "local variable 'X' referenced before assignment\n",
      "6 1\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "6 2\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "6 3\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "6 4\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "6 5\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "7 0\n",
      "local variable 'X' referenced before assignment\n",
      "7 1\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "7 2\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "7 3\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "7 4\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "7 5\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "7 6\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "1 0\n",
      "local variable 'X' referenced before assignment\n",
      "2 0\n",
      "local variable 'X' referenced before assignment\n",
      "2 1\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "3 0\n",
      "local variable 'X' referenced before assignment\n",
      "3 1\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "3 2\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "4 0\n",
      "local variable 'X' referenced before assignment\n",
      "4 1\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "4 2\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "4 3\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "5 0\n",
      "local variable 'X' referenced before assignment\n",
      "5 1\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "5 2\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "5 3\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "5 4\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "6 0\n",
      "local variable 'X' referenced before assignment\n",
      "6 1\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "6 2\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "6 3\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "6 4\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "6 5\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "7 0\n",
      "local variable 'X' referenced before assignment\n",
      "7 1\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "7 2\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "7 3\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "7 4\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "7 5\n",
      "0\n",
      "local variable 'X' referenced before assignment\n",
      "7 6\n",
      "0\n",
      "local variable 'X' referenced before assignment\n"
     ]
    }
   ],
   "source": [
    "path = \"balanced\"\n",
    "train_extra_data, train_extra_label = get_extra_data(path, train = True, one_vs_all = False)\n",
    "eval_extra_data, eval_extra_label = get_extra_data(path, train = False, one_vs_all = False)\n",
    "X = train_extra_data.reshape(-1, 4*16)\n",
    "X_val = eval_extra_data.reshape(-1,  4*16)\n",
    "#all_final_features = np.concatenate((train_feat_CSP, X), axis= 1)\n",
    "#all_final_features_val = np.concatenate((eval_feature_CSP, X_val), axis= 1)\n",
    "for k in [time_windows[0:1], time_windows]:\n",
    "    for i in range(8):\n",
    "        for j in range(i):\n",
    "            try:\n",
    "                print(i,j)\n",
    "                train_data, train_label = get_data_f(i,j, i+j,path, train = True, one_vs_all = False)\n",
    "                eval_data, eval_label = get_data_f(i,j,path, train = False, one_vs_all = False)\n",
    "                start = time.time()\n",
    "                print('ok')\n",
    "                train_data = train_data.transpose((1,0,2))[j:i].transpose((1, 0, 2))\n",
    "                eval_data = eval_data.transpose((1,0,2))[j:i].transpose((1, 0, 2))\n",
    "                print(train_data.shape, train_label.shape)\n",
    "                w, train_feat_CSP = get_features(train_data, train_label, k, useCSP=False)\n",
    "                eval_feature_CSP = extract_feature(eval_data, w, k, time_windows)\n",
    "                print(\"train\")\n",
    "                clf = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "                clf.fit(train_feat_CSP, train_label)\n",
    "                labels_pred = clf.predict(eval_feature_CSP)\n",
    "                CM = confusion_matrix(eval_label, labels_pred)\n",
    "                Acc = accuracy_score(eval_label, labels_pred)\n",
    "                F1 = f1_score(eval_label, labels_pred, average='macro')\n",
    "                print(CM, Acc, F1)\n",
    "                print(time.time() - start)\n",
    "                start = time.time()\n",
    "                print(\"add features\")\n",
    "                all_final_features = np.concatenate((train_feat_CSP, X), axis= 1)\n",
    "                all_final_features_val = np.concatenate((eval_feature_CSP, X_val), axis= 1)\n",
    "                print(all_final_features.shape, train_label.shape)\n",
    "                clf = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "                clf.fit(all_final_features, train_label)\n",
    "                labels_pred = clf.predict(all_final_features_val)\n",
    "                CM = confusion_matrix(eval_label, labels_pred)\n",
    "                Acc = accuracy_score(eval_label, labels_pred)\n",
    "                F1 = f1_score(eval_label, labels_pred, average='macro')\n",
    "                print(CM, Acc, F1)\n",
    "                print(time.time() - start)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

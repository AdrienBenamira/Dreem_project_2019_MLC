
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Rapport - part1}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Machine Learning and Classification : Rapport de
projet}\label{machine-learning-and-classification-rapport-de-projet}

\emph{Par BENAMIRA Adrien, CARRIÉ Hanaé et DEVILLERS Benjamin}

\subsection{PLAN :}\label{plan}

(donné par le prof.)\\
a. L'étape de prétraitement des données\\
b. L'extraction de features (s'il en est)\\
c. Une description théorique (et aussi pédagogique que possible) du
modèle utilisé\\
d. Le protocole de validation croisée utilisée\\
e. Présentation des résultats, figures, etc....

    \subsection{1. Introduction}\label{introduction}

    Ce projet a pour but de proposer d'utiliser les données du bandeau de
Dreem pour évaluer le stade de sommeil sur des périodes de 30 secondes
de signaux biophysiologiques.

Le sommeil joue un rôle vital dans la santé et le bien-être d'une
personne. Le sommeil progresse selon des cycles qui impliquent plusieurs
stades de sommeil : réveil, sommeil léger, sommeil profond, sommeil
paradoxal et à chaque stades du sommeil sont associés différentes
fonctions physiologiques. Déterminer ce stade est indispensable pour
diagnostiquer les troubles du sommeil. Aujourd'hui, les études
polysomnographiques sont menées dans un hôpital ou dans un laboratoire
du sommeil etde multiples signaux physiologiques sont enregistrés comme
l'électroencéphalogramme, l'électrocardiogramme, etc. L'évaluation de
l'étape du sommeil est ensuite effectué visuellement par un expert sur
les époques de 30 secondes d'enregistrement des signaux. Le graphique
qui en résulte est appelé hypnogramme.

Le bandeau Dreem permet de faire de la polysomnographie à domicile grâce
à trois types de capteurs : électroencéphalogramme (EEG), pouls et
signaux accélérométriques. Les données mesurées par Dreem ont été
labélisés par un expert en sommeil qualifié.

Le but de cet compétition Kaggle est de développer un algorithme pour
classifier les phases de sommeil Wake, N1, N2, N3 et REM sur des
fenêtres de 30 secondes de données brutes. Elles comprennent 7 canaux
eegs en position frontale et occipitale, 1 canal infrarouge d'oxymètre
de pouls et 3 canaux accéléromètres (x, y et z).

    \subsection{2. Chargement des données et
pré-traitements}\label{chargement-des-donnuxe9es-et-pruxe9-traitements}

    \subsubsection{2.1. Chargement des
données}\label{chargement-des-donnuxe9es}

Les données sont sauvegardée au format HDF5. Ce format est
particulièrement intéressant car il permet de stocker des lourdes
données compréssées en un dataset. La bibliothèque Python \texttt{h5py}
permet de pouvoir les lire ces données facilement de la même manière que
nous utiliserions un tableau \texttt{Numpy}.

La class python \texttt{DreemDataset} dans le package
\texttt{tools.data} permet de récupérer et manipuler les datasets. Comme
nous voulons effectuer des prétraitements sur nos données et les stocker
pour pouvoir rapidement appliquer les algorithmes d'apprentissage sur
nos données traitées, nous avons conçu cette classe de sorte que nous
puissions appliquer puis sauvegarder.

Nous avons choisis de les sauvegarder ensuite dans des tableaux
\texttt{npy} pour des questions de performances d'execution.

La classe python \texttt{DreemDatasets} permet quant à elle de générer
un "dev set" et "train set" à partir des données. Celle-ci retourne un
couple d'instance de \texttt{DreemDataset}. Puis que les différentes
classes ne sont pas équilibrées, nous pouvons également ne récupérer
qu'un sous-ensemble du dataset équilibré.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{k+kn}{from} \PY{n+nn}{tools}\PY{n+nn}{.}\PY{n+nn}{data} \PY{k}{import} \PY{n}{DreemDatasets}
\end{Verbatim}


    Les paramètres sont :

\begin{itemize}
\tightlist
\item
  chemin vers les données
\item
  chemin vers les cibles
\item
  \texttt{keep\_datasets} Liste des datasets à garder, parmi les
  suivants :

  \begin{itemize}
  \tightlist
  \item
    \texttt{eeg\_1} - EEG in frontal position sampled at 50 Hz
    -\textgreater{} 1500 values
  \item
    \texttt{eeg\_2} - EEG in frontal position sampled at 50 Hz
    -\textgreater{} 1500 values
  \item
    \texttt{eeg\_3} - EEG in frontal position sampled at 50 Hz
    -\textgreater{} 1500 values
  \item
    \texttt{eeg\_4} - EEG in frontal-occipital position sampled at 50 Hz
    -\textgreater{} 1500 values
  \item
    \texttt{eeg\_5} - EEG in frontal-occipital position sampled at 50 Hz
    -\textgreater{} 1500 values
  \item
    \texttt{eeg\_6} - EEG in frontal-occipital position sampled at 50 Hz
    -\textgreater{} 1500 values
  \item
    \texttt{eeg\_7} - EEG in frontal-occipital position sampled at 50 Hz
    -\textgreater{} 1500 values
  \item
    \texttt{accelerometer\_x} - Accelerometer along x axis sampled at 10
    Hz -\textgreater{} 300 values
  \item
    \texttt{accelerometer\_y} - Accelerometer along y axis sampled at 10
    Hz -\textgreater{} 300 values
  \item
    \texttt{accelerometer\_z} - Accelerometer along z axis sampled at 10
    Hz -\textgreater{} 300 values
  \item
    \texttt{pulse\_oximeter\_infrared} - Pulse oximeter infrared channel
    sampled at 10 Hz -\textgreater{} 300 values
  \end{itemize}
\item
  \texttt{split\_train\_val} Pourcentage pour partager le train set et
  validation set
\item
  \texttt{seed} Une seed pour la reproductibilité
\item
  \texttt{balance\_data} si vrai, équilibre le dataset pour avoir le
  nombre de donnée par classe
\item
  \texttt{size} Une taille maximale pour le dataset (si non renseignée,
  tout le dataset)
\item
  \texttt{transforms} des transformations à appliquer aux données (voir
  la partie transformation)
\item
  \texttt{transforms\_val} si renseignée, \texttt{transforms} sera pour
  le train et \texttt{transforms\_val} pour la validation. Sinon, même
  transformation que \texttt{transforms}.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{n}{train\PYZus{}set}\PY{p}{,} \PY{n}{val\PYZus{}set} \PY{o}{=} \PY{n}{DreemDatasets}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dataset/train.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dataset/train\PYZus{}y.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                            \PY{n}{split\PYZus{}train\PYZus{}val}\PY{o}{=}\PY{l+m+mf}{0.8}\PY{p}{,} 
                                            \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} 
                                            \PY{n}{keep\PYZus{}datasets}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{eeg\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{train\PYZus{}set}\PY{o}{.}\PY{n}{load\PYZus{}data}\PY{p}{(}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Load les données en mémoire}
         
         \PY{n}{val\PYZus{}set}\PY{o}{.}\PY{n}{load\PYZus{}data}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Ne pas oublier de fermer les datasets}
         \PY{c+c1}{\PYZsh{} Ne ferme que les fichiers h5. Si .load\PYZus{}data() a été appelé, on a toujours accès aux données !}
         \PY{n}{train\PYZus{}set}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
         \PY{n}{val\PYZus{}set}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Loading data in memory{\ldots}
5412 in 1 datasets to load
Loading dataset eeg\_1 {\ldots}
Done.
Loading data in memory{\ldots}
1353 in 1 datasets to load
Loading dataset eeg\_1 {\ldots}
Done.

    \end{Verbatim}

    \paragraph{2.1.1. Récupération des
données}\label{ruxe9cupuxe9ration-des-donnuxe9es}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{n}{data\PYZus{}50hz}\PY{p}{,} \PY{n}{data\PYZus{}10hz}\PY{p}{,} \PY{n}{target} \PY{o}{=} \PY{n}{train\PYZus{}set}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}  \PY{c+c1}{\PYZsh{} Une valeur}
         
         \PY{c+c1}{\PYZsh{} Dimension nb\PYZus{}datasets x tailles\PYZus{}features}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{data\PYZus{}50hz}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         
         \PY{n}{data\PYZus{}50hz}\PY{p}{,} \PY{n}{data\PYZus{}10hz}\PY{p}{,} \PY{n}{targets} \PY{o}{=} \PY{n}{train\PYZus{}set}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}  \PY{c+c1}{\PYZsh{} 10 valeurs}
         \PY{c+c1}{\PYZsh{} Dimension nb\PYZus{}datasets x nb\PYZus{}elements (=10) x tailles\PYZus{}features}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{data\PYZus{}50hz}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
(1, 1500)
(1, 10, 1500)

    \end{Verbatim}

    \paragraph{2.1.2. Enregistrer un dataset}\label{enregistrer-un-dataset}

On peut utiliser la méthode \texttt{save\_data} pour enregistrer dans un
\texttt{.npy}. Pour des questions de mémoire, il y aura un fichier par
dataset (ex, si on choisit d'ouvrir \texttt{{[}"eeg\_1",\ "eeg\_2"{]}}
alors il y aura deux fichiers).

Attention, le nom des fichiers ne peut pas être précisé, seulement un
dossier parent.

Par exemple, avec le \texttt{save\_data("dataset/test")}, on aura les
fichiers : - \texttt{dataset/test/eeg\_1.npy} -
\texttt{dataset/test/eeg\_2.npy}

On peut préciser le chemin vers un nouveau dossier, celui-ci sera créé.

La sauvegarde enregistre les données \textbf{transformée}. Les données
brutes ne sont pas enregistrées.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{n}{train\PYZus{}set}\PY{p}{,} \PY{n}{val\PYZus{}set} \PY{o}{=} \PY{n}{DreemDatasets}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dataset/train.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dataset/train\PYZus{}y.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                            \PY{n}{split\PYZus{}train\PYZus{}val}\PY{o}{=}\PY{l+m+mf}{0.8}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{keep\PYZus{}datasets}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{eeg\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{train\PYZus{}set}\PY{o}{.}\PY{n}{save\PYZus{}data}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dataset/sauvegarde/train}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Attention, pas de / à la fin !}
         \PY{n}{val\PYZus{}set}\PY{o}{.}\PY{n}{save\PYZus{}data}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dataset/sauvegarde/val}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{n}{train\PYZus{}set}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
         \PY{n}{val\PYZus{}set}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Saving into dataset/sauvegarde/train {\ldots}
Loading dataset eeg\_1 {\ldots}
Saved.
Saving into dataset/sauvegarde/val {\ldots}
Loading dataset eeg\_1 {\ldots}
Saved.

    \end{Verbatim}

    Si l'on a déjà enregistré le dataset, on peut utiliser
\texttt{load\_data} en précisant le dossier dans lequel sont tous les
fichiers

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{n}{train\PYZus{}set}\PY{p}{,} \PY{n}{val\PYZus{}set} \PY{o}{=} \PY{n}{DreemDatasets}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dataset/train.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dataset/train\PYZus{}y.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                            \PY{n}{split\PYZus{}train\PYZus{}val}\PY{o}{=}\PY{l+m+mf}{0.8}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{keep\PYZus{}datasets}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{eeg\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Cela remplace les données données des fichiers h5.}
         \PY{n}{train\PYZus{}set}\PY{o}{.}\PY{n}{load\PYZus{}data}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dataset/sauvegarde/train}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Attention, pas de / à la fin !}
         \PY{n}{val\PYZus{}set}\PY{o}{.}\PY{n}{load\PYZus{}data}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dataset/sauvegarde/val}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{n}{train\PYZus{}set}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
         \PY{n}{val\PYZus{}set}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{data\PYZus{}50hz}\PY{p}{,} \PY{n}{data\PYZus{}10hz}\PY{p}{,} \PY{n}{target} \PY{o}{=} \PY{n}{train\PYZus{}set}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{data\PYZus{}50hz}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Loading data in memory{\ldots}
5412 in 1 datasets to load
Loading dataset eeg\_1 {\ldots}
Done.
Loading data in memory{\ldots}
1353 in 1 datasets to load
Loading dataset eeg\_1 {\ldots}
Done.
(1, 1500)

    \end{Verbatim}

    \paragraph{2.1.3. Test set}\label{test-set}

On peut faire la même chose avec le test set qu'avec les autres données.

Attention à bien appeler \texttt{init()}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{k+kn}{from} \PY{n+nn}{tools}\PY{n+nn}{.}\PY{n+nn}{data} \PY{k}{import} \PY{n}{DreemDataset}
         
         \PY{n}{test\PYZus{}set} \PY{o}{=} \PY{n}{DreemDataset}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dataset/test.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{keep\PYZus{}datasets}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{eeg\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{init}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Rappel : charge en mémoire. On peut également charget un fichier en précisant un chemin}
         \PY{n}{test\PYZus{}set}\PY{o}{.}\PY{n}{load\PYZus{}data}\PY{p}{(}\PY{p}{)}  
         
         \PY{n}{test\PYZus{}set}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Loading data in memory{\ldots}
37439 in 1 datasets to load
Loading dataset eeg\_1 {\ldots}
Done.

    \end{Verbatim}

    \subsubsection{2.2. Pré-traitements}\label{pruxe9-traitements}

\paragraph{2.2.1. Introduction :
Transformations}\label{introduction-transformations}

Une transformation est ce qui nous permet de faire nos pré-traitements
sur nos données. Pour effectuer des transformations sur nos données,
nous devons definir un dictionnaire :

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{k}{def} \PY{n+nf}{transformation\PYZus{}eeg\PYZus{}1}\PY{p}{(}\PY{n}{batch\PYZus{}signals}\PY{p}{,} \PY{n}{batch\PYZus{}targets}\PY{p}{)}\PY{p}{:}
             \PY{k}{return} \PY{n}{batch\PYZus{}signals}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}
         
         \PY{n}{transformations} \PY{o}{=} \PY{p}{\PYZob{}}
             \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{eeg\PYZus{}1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{transformation\PYZus{}eeg\PYZus{}1}
         \PY{p}{\PYZcb{}}
\end{Verbatim}


    Prenant en clé le nom du dataset sur lequel effectuer la transformation
et en valeur une fonction prenant 2 paramètres :

\begin{itemize}
\tightlist
\item
  un batch du signal de taille (batch, taille\_signal)
\item
  les "targets" qui correspondent aux classes de sommeils que nous
  devons prédire pour le batch en question.
\end{itemize}

Ces deux paramètres peuvent être utiles pour nos différentes fonctions
de pré-processing.

La transformation doit retourner le batch modifié.

Dans l'exemple ci-dessus, nous appliquerons une transformation au
dataset \texttt{eeg\_1} qui tranformera les signaux en seulement la
première valeur du signal.

    \paragraph{2.2.2. Extraction en bandes}\label{extraction-en-bandes}

D'après
{[}\href{http://www.fmed.edu.uy/sites/www.labsueno.fmed.edu.uy/files/9.Diagnostico-polisomnogr\%C3\%A1fico.pdf}{1:
Malhotra, R. K., \& Avidan, A. Y. (2014). Sleep stages and scoring
technique. Atlas of Sleep Medicine, 77-99.}{]} et
{[}\href{https://www.mdpi.com/1099-4300/18/9/272}{2: Aboalayon, K.,
Faezipour, M., Almuhammadi, W., \& Moslehpour, S. (2016). Sleep stage
classification using EEG signal analysis: a comprehensive survey and new
investigation. Entropy, 18(9), 272.}{]}, les électroencéphalogrames
(eegs) peuvent être séparés en bande de fréquences contenant différentes
informations :

\begin{itemize}
\tightlist
\item
  Bande \(\delta\) : de 0 à 4 Hz,
\item
  bande \(\theta\) : de 4 à 8 Hz,
\item
  bande \(\alpha\) : de 8 à 13 Hz,
\item
  bande \(\beta\) : de 13 à 38 Hz,
\item
  bande \(\gamma\) : de 38 à 42 Hz.
\end{itemize}

Cependant nos eegs sont échantillonés à 50 Hz. Le thèorème de Shannon
(Sampling theorem) indique que nous ne pourrons récupérer que des
informations sur une bande de fréquence entre 0 et 25 Hz.

Nous ne considerons donc pour cela pas la bande \(\gamma\) et gardon la
bande \(\beta\) que de 13 à 22 Hz.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{k+kn}{from} \PY{n+nn}{preprocessing}\PY{n+nn}{.}\PY{n+nn}{signals} \PY{k}{import} \PY{n}{ExtractBands}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{n}{extract\PYZus{}bands} \PY{o}{=} \PY{n}{ExtractBands}\PY{p}{(}\PY{n}{bands}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{*}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{} toutes les bandes (de delta à beta)}
\end{Verbatim}


    \texttt{extract\_bands} est une transformation que nous pouvons
appliquer aux eegs et qui sort des signaux extraits en bande :

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{n}{train\PYZus{}set}\PY{p}{,} \PY{n}{val\PYZus{}set} \PY{o}{=} \PY{n}{DreemDatasets}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dataset/train.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dataset/train\PYZus{}y.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                            \PY{n}{split\PYZus{}train\PYZus{}val}\PY{o}{=}\PY{l+m+mf}{0.8}\PY{p}{,} 
                                            \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} 
                                            \PY{n}{keep\PYZus{}datasets}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{eeg\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
                                            \PY{n}{transforms}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{eeg\PYZus{}1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{extract\PYZus{}bands}\PY{p}{\PYZcb{}}\PY{p}{)}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{train\PYZus{}set}\PY{o}{.}\PY{n}{load\PYZus{}data}\PY{p}{(}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Load les données en mémoire}
         
         \PY{n}{val\PYZus{}set}\PY{o}{.}\PY{n}{load\PYZus{}data}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Ne pas oublier de fermer les datasets}
         \PY{c+c1}{\PYZsh{} Ne ferme que les fichiers h5. Si .load\PYZus{}data() a été appelé, on a toujours accès aux données !}
         \PY{n}{train\PYZus{}set}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
         \PY{n}{val\PYZus{}set}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{data\PYZus{}50hz}\PY{p}{,} \PY{n}{data\PYZus{}10hz}\PY{p}{,} \PY{n}{target} \PY{o}{=} \PY{n}{train\PYZus{}set}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{data\PYZus{}50hz}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Loading data in memory{\ldots}
5412 in 1 datasets to load
Loading dataset eeg\_1 {\ldots}
Apply transformations{\ldots}
Applied.
Done.
Loading data in memory{\ldots}
1353 in 1 datasets to load
Loading dataset eeg\_1 {\ldots}
Apply transformations{\ldots}
Applied.
Done.
(1, 4, 1500)

    \end{Verbatim}

    Noter la dimension "4" ajoutée qui correspond aux 4 bandes de fréquence.

    \paragraph{2.2.3. Features plus
avancées}\label{features-plus-avancuxe9es}

    Nous avons ensuite ajouté différents pré-traitements de base sur nos
features tel que :

\begin{itemize}
\tightlist
\item
  \texttt{min} valeur min du signal,
\item
  \texttt{max} valeur max du signal,
\item
  \texttt{frequency} fréquence du signal,
\item
  \texttt{energy} énergie du signal.
\end{itemize}

Puis nous avons essayé les features proposés par
{[}\href{https://www.mdpi.com/1099-4300/18/9/272}{2}{]} : * \texttt{mmd}
distance minimum-maximum * \texttt{esis} correspond à l'énergie et
"vitesse" du signal

De plus, comme expliqué par
{[}\href{http://www.fmed.edu.uy/sites/www.labsueno.fmed.edu.uy/files/9.Diagnostico-polisomnogr\%C3\%A1fico.pdf}{1}{]},
section "\emph{Stages of Sleep}", nous pouvons distinguer différentes
étapes du sommeil par différentes prédominances "d'utilisation" d'une
bande : * \texttt{band-usage} est la feature qui indique, sur une
fenêtre glissante, la bande qui est a la plus grande valeur sur la
fenêtre parmi toutes les bandes. Cela donne une indication de la bande
prédominante sur la fenêtre.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{c+c1}{\PYZsh{} Transformation pour ces featyres}
         \PY{k+kn}{from} \PY{n+nn}{preprocessing}\PY{n+nn}{.}\PY{n+nn}{features} \PY{k}{import} \PY{n}{ExtractFeatures}
         
         \PY{c+c1}{\PYZsh{} Extract features peut directement extraire les bandes}
         \PY{n}{extract\PYZus{}features} \PY{o}{=} \PY{n}{ExtractFeatures}\PY{p}{(}\PY{n}{bands}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{*}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{features}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mmd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{esis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{n}{train\PYZus{}set}\PY{p}{,} \PY{n}{val\PYZus{}set} \PY{o}{=} \PY{n}{DreemDatasets}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dataset/train.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dataset/train\PYZus{}y.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                            \PY{n}{split\PYZus{}train\PYZus{}val}\PY{o}{=}\PY{l+m+mf}{0.8}\PY{p}{,} 
                                            \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} 
                                            \PY{n}{keep\PYZus{}datasets}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{eeg\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
                                            \PY{n}{transforms}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{eeg\PYZus{}1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{extract\PYZus{}features}\PY{p}{\PYZcb{}}\PY{p}{)}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{train\PYZus{}set}\PY{o}{.}\PY{n}{load\PYZus{}data}\PY{p}{(}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Load les données en mémoire}
         
         \PY{n}{val\PYZus{}set}\PY{o}{.}\PY{n}{load\PYZus{}data}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Ne pas oublier de fermer les datasets}
         \PY{c+c1}{\PYZsh{} Ne ferme que les fichiers h5. Si .load\PYZus{}data() a été appelé, on a toujours accès aux données !}
         \PY{n}{train\PYZus{}set}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
         \PY{n}{val\PYZus{}set}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{data\PYZus{}50hz}\PY{p}{,} \PY{n}{data\PYZus{}10hz}\PY{p}{,} \PY{n}{target} \PY{o}{=} \PY{n}{train\PYZus{}set}\PY{p}{[}\PY{p}{:}\PY{p}{]}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{data\PYZus{}50hz}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Loading data in memory{\ldots}
5412 in 1 datasets to load
Loading dataset eeg\_1 {\ldots}
Apply transformations{\ldots}
Applied.
Done.
Loading data in memory{\ldots}
1353 in 1 datasets to load
Loading dataset eeg\_1 {\ldots}
Apply transformations{\ldots}
Applied.
Done.
(1, 4, 5412, 2)

    \end{Verbatim}

    On voit ici que la dimension est de 1 (=1 dataset) x 4 (=4 bandes) x
nombre de signaux x 2 (=2 features).

    \subsubsection{2.3. Etat de l'art : Caractéristiques temporelles et
spectrales à grande
échelle}\label{etat-de-lart-caractuxe9ristiques-temporelles-et-spectrales-uxe0-grande-uxe9chelle}

    La classification multi-classes précise, rapide et fiable de signaux
d'électroencéphalographie (EEG) est un défi pour le développement de
systèmes d'interface cerveau-ordinateur (MI-BCI) d'imagerie motrice.
C'est un domaine assez actif, comme le montre les différentes concours
tel que \href{http://bnci-horizon-2020.eu/database/data-sets}{le
concours BCI Competition IV C. Brunner et al.}

Après un état de l'art efficace, plusieurs approches auraient pu etre
envisagé :

A commencé par une approche monodirectionelle concentré sur les EEGS : à
partir d'une seule courbe de l'un des EEG de 30secondes déterminé sa
classe et répété l'opération pour les 7 courbes (reférence :
\href{https://ieeexplore.ieee.org/document/7362882}{S. Sakhav and al.}
). Ou bien une approche multidirectionnelle concentré sur l'extraction
de caractéristique qui maximisent la discriminabilité entre deux
classes.

Nous nous sommes attaché à exploré la seconde voie et il semblerait que
la méthode la plus efficace soit celle propsosé par
\href{https://arxiv.org/abs/1806.06823}{Hersche et al.}

L'idée est d'utiliser le modèle spatial commun (CSP) et celui avec la
géométrie de Riemannian pour l'étendre aux cas de multifenêtrage
temporelle et spectrale.

    \paragraph{2.3.1 Filter Bank Common Spatial Pattern
(FBCSP)}\label{filter-bank-common-spatial-pattern-fbcsp}

    L'algorithme des modèles spatiaux communs (CSP) apprend des filtres
spatiaux qui maximisent la discriminabilité entre deux classes
(\href{https://ieeexplore.ieee.org/document/5593210}{F. Lotte and al.}).
Ses performances dépendent fortement des bandes de fréquences
opérationnelles considérées. Ainsi, dans la plupart des applications,
les données sont d'abord divisées en plusieurs bandes de fréquences,
puis spatialement filtrées. C'est ce que l'on appelle la configuration
spatiale commune des bancs de filtres.

Dans notre cas nous utiliserons les paramètres suivants :

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{k+kn}{import} \PY{n+nn}{os}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         \PY{k+kn}{from} \PY{n+nn}{tools}\PY{n+nn}{.}\PY{n+nn}{filters} \PY{k}{import} \PY{n}{load\PYZus{}filterbank}
         \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{k+kn}{from} \PY{n+nn}{tools}\PY{n+nn}{.}\PY{n+nn}{csp} \PY{k}{import} \PY{n}{generate\PYZus{}projection}\PY{p}{,} \PY{n}{generate\PYZus{}eye}\PY{p}{,} \PY{n}{extract\PYZus{}feature}
         \PY{k+kn}{from} \PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{Compose}\PY{p}{,} \PY{n}{ExtractBands}\PY{p}{,} \PY{n}{ExtractSpectrum}
         
         \PY{n}{fs} \PY{o}{=} \PY{l+m+mf}{50.}  \PY{c+c1}{\PYZsh{} sampling frequency}
         \PY{n}{NO\PYZus{}channels} \PY{o}{=} \PY{l+m+mi}{7}  \PY{c+c1}{\PYZsh{} number of EEG channels}
         \PY{n}{NO\PYZus{}riem} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{NO\PYZus{}channels} \PY{o}{*} \PY{n}{NO\PYZus{}channels} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{/} \PY{l+m+mi}{2}  \PY{c+c1}{\PYZsh{} Total number of CSP feature per band and timewindow}
         \PY{n}{bw} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{13}\PY{p}{,} \PY{l+m+mi}{22}\PY{p}{]}\PY{p}{)}
         \PY{n}{ftype} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{butter}\PY{l+s+s1}{\PYZsq{}}  \PY{c+c1}{\PYZsh{} \PYZsq{}fir\PYZsq{}, \PYZsq{}butter\PYZsq{}}
         \PY{n}{forder} \PY{o}{=} \PY{l+m+mi}{2}  \PY{c+c1}{\PYZsh{} 4}
         \PY{n}{filter\PYZus{}bank} \PY{o}{=} \PY{n}{load\PYZus{}filterbank}\PY{p}{(}\PY{n}{bw}\PY{p}{,} \PY{n}{fs}\PY{p}{,} \PY{n}{order}\PY{o}{=}\PY{n}{forder}\PY{p}{,} \PY{n}{max\PYZus{}freq}\PY{o}{=}\PY{l+m+mi}{23}\PY{p}{,} \PY{n}{ftype}\PY{o}{=}\PY{n}{ftype}\PY{p}{)}  \PY{c+c1}{\PYZsh{} get filterbank coeffs}
         \PY{n}{time\PYZus{}windows\PYZus{}flt} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{30}\PY{p}{]}\PY{p}{,}
                                      \PY{p}{[}\PY{l+m+mi}{15}\PY{p}{,} \PY{l+m+mi}{30}\PY{p}{]}\PY{p}{,}
                                      \PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{]}\PY{p}{,}
                                      \PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{]}\PY{p}{,}
                                      \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{]}\PY{p}{,}
                                      \PY{p}{[}\PY{l+m+mi}{15}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{]}\PY{p}{,}
                                      \PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{]}\PY{p}{,}
                                      \PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{]}\PY{p}{,}
                                      \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}
         \PY{p}{]}\PY{p}{)} \PY{o}{*} \PY{n}{fs}
         \PY{n}{time\PYZus{}windows} \PY{o}{=} \PY{n}{time\PYZus{}windows\PYZus{}flt}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{int}\PY{p}{)}
         \PY{n}{NO\PYZus{}bands} \PY{o}{=} \PY{n}{filter\PYZus{}bank}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n}{NO\PYZus{}csp} \PY{o}{=} \PY{l+m+mi}{24}  \PY{c+c1}{\PYZsh{} Total number of CSP feature per band and timewindow}
         \PY{n}{useCSP} \PY{o}{=} \PY{k+kc}{False}
         
         \PY{n}{Image}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{figures}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{fig1.png}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}47}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_34_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    Cette figure est extraite de
\href{https://arxiv.org/abs/1806.06823}{Hersche et al.} Pour nous, le
temps varie de 0 à 30 secondes et la fréquence de 2 à 22 Herts. Le code
pour appliquer ce préprocess est le suivant :

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k}{def} \PY{n+nf}{get\PYZus{}features}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{label}\PY{p}{,} \PY{n}{time\PYZus{}windows}\PY{p}{,} \PY{n}{useCSP} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}\PY{p}{:}
            \PY{k}{if} \PY{n}{useCSP}\PY{p}{:}
                \PY{n}{w} \PY{o}{=} \PY{n}{generate\PYZus{}projection}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{label}\PY{p}{,} \PY{n}{NO\PYZus{}csp}\PY{p}{,} \PY{n}{filter\PYZus{}bank}\PY{p}{,} \PY{n}{time\PYZus{}windows}\PY{p}{,} \PY{n}{NO\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
            \PY{k}{else}\PY{p}{:}
                \PY{n}{w} \PY{o}{=} \PY{n}{generate\PYZus{}eye}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{label}\PY{p}{,} \PY{n}{filter\PYZus{}bank}\PY{p}{,} \PY{n}{time\PYZus{}windows}\PY{p}{)}
            \PY{n}{feature\PYZus{}mat} \PY{o}{=} \PY{n}{extract\PYZus{}feature}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{filter\PYZus{}bank}\PY{p}{,} \PY{n}{time\PYZus{}windows}\PY{p}{)}
            \PY{k}{return}\PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{feature\PYZus{}mat}\PY{p}{)}
        \PY{c+c1}{\PYZsh{}w, train\PYZus{}feat\PYZus{}CSP = get\PYZus{}features(train\PYZus{}data, train\PYZus{}label, time\PYZus{}windows, useCSP)}
        \PY{c+c1}{\PYZsh{}test\PYZus{}feature\PYZus{}CSP = extract\PYZus{}feature(test\PYZus{}data, w, filter\PYZus{}bank, time\PYZus{}windows)}
\end{Verbatim}


    \paragraph{2.3.2 Riemannian covariance
features}\label{riemannian-covariance-features}

    Nous n'irons pas dans les détails puisque nous l'avons implémenté et
essayé mais ces caractéristiques donnent de (légers) moins bons
résultats que les CSP features. Vous pourrez vous refairez au paragraphe
4 de la publication \href{https://arxiv.org/abs/1806.06823}{Hersche et
al.} pour plus de détails.

Voici toutefois le code à déployer pour utiliser ces features :

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kn}{from} \PY{n+nn}{models}\PY{n+nn}{.}\PY{n+nn}{riemannian\PYZus{}multiscale} \PY{k}{import} \PY{n}{riemannian\PYZus{}multiscale}
        \PY{n}{riemann} \PY{o}{=} \PY{n}{riemannian\PYZus{}multiscale}\PY{p}{(}\PY{n}{filter\PYZus{}bank}\PY{p}{,} \PY{n}{time\PYZus{}windows}\PY{p}{,} \PY{n}{riem\PYZus{}opt}\PY{o}{=}\PY{n}{riem\PYZus{}opt}\PY{p}{,} \PY{n}{rho}\PY{o}{=}\PY{n}{rho}\PY{p}{,} \PY{n}{vectorized}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        \PY{n}{features\PYZus{}CSP\PYZus{}train\PYZus{}R} \PY{o}{=} \PY{n}{riemann}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}data}\PY{p}{)}
        \PY{n}{features\PYZus{}CSP\PYZus{}test\PYZus{}R} \PY{o}{=} \PY{n}{riemann}\PY{o}{.}\PY{n}{features}\PY{p}{(}\PY{n}{test\PYZus{}data}\PY{p}{)}
\end{Verbatim}


    \subsubsection{2.4.3 Comparaison de toutes les
features}\label{comparaison-de-toutes-les-features}

    Notre base de données est déséquilibré :

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}95}]:} \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
         \PY{n}{label\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dataset/data/train\PYZus{}label.npy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{bins}\PY{p}{,} \PY{n}{patches}\PY{p}{)} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{label\PYZus{}train}\PY{p}{,} \PY{n}{density}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{5}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{percentage per stage: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{n}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_42_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
percentage per stage:  [ 9.45180078  3.53365196 44.74392123 14.96774531 27.30288072]

    \end{Verbatim}

    Nous avons donc tester toutes nos features sur deux approches : un
subdataset équilibré et sur tout le dataset. Pour chaque test nous
utilisons la même méthode de classification :

clf = RandomForestClassifier(n\_estimators=700,max\_features='auto',
random\_state=0)

Nous obtenons les résultats suivant (cf All\_features\_method.ipynb et
All\_features\_method\_balanced.ipynb) :

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
         
         \PY{n}{df\PYZus{}all} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Différents models sur tout le dataset}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Min \PYZhy{} max \PYZhy{} freq \PYZhy{} energy on pulse et accelerometre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Min \PYZhy{} max \PYZhy{} freq \PYZhy{} energy on eegs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Min \PYZhy{} max \PYZhy{} freq \PYZhy{} energy on all}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ Min \PYZhy{} max \PYZhy{} freq \PYZhy{} energy on all + Riemann}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ Min \PYZhy{} max \PYZhy{} freq \PYZhy{} energy on all + CSP}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
          \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F1\PYZhy{}score val}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{l+m+mf}{0.518054836809841}\PY{p}{,} \PY{l+m+mf}{0.5632674016257432}\PY{p}{,} \PY{l+m+mf}{0.5893988863012602}\PY{p}{,} \PY{l+m+mf}{0.6046064049161487}\PY{p}{,} \PY{l+m+mf}{0.6178183804270211}\PY{p}{]}\PY{p}{,} 
          \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Time}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{p}{[}\PY{l+m+mi}{193}\PY{p}{,}\PY{l+m+mi}{308}\PY{p}{,}\PY{l+m+mi}{408}\PY{p}{,} \PY{l+m+mi}{2500}\PY{p}{,} \PY{l+m+mi}{3000}\PY{p}{]}\PY{p}{\PYZcb{}}\PY{p}{)}
         
         \PY{n}{df\PYZus{}all}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}23}]:}                Différents models sur tout le dataset  F1-score val  Time
         0  Min - max - freq - energy on pulse et accelero{\ldots}      0.518055   193
         1                  Min - max - freq - energy on eegs      0.563267   308
         2                   Min - max - freq - energy on all      0.589399   408
         3         Min - max - freq - energy on all + Riemann      0.604606  2500
         4             Min - max - freq - energy on all + CSP      0.617818  3000
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{n}{df\PYZus{}balanced} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Différents models sur un subdataset balanced}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Min \PYZhy{} max \PYZhy{} freq \PYZhy{} energy on pulse et accelerometre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                                                                    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Min \PYZhy{} max \PYZhy{} freq \PYZhy{} energy on eegs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                                                                    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Min \PYZhy{} max \PYZhy{} freq \PYZhy{} energy on all}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                                                                    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ Min \PYZhy{} max \PYZhy{} freq \PYZhy{} energy on all + Riemann}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                                                                    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ Min \PYZhy{} max \PYZhy{} freq \PYZhy{} energy on all + CSP}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
          \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F1\PYZhy{}score val}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{l+m+mf}{0.5638819160728847}\PY{p}{,} \PY{l+m+mf}{0.6254347640147918}\PY{p}{,} \PY{l+m+mf}{0.6397347891229647}\PY{p}{,} \PY{l+m+mf}{0.6397853700614984}\PY{p}{,} \PY{l+m+mf}{0.639209140962282}\PY{p}{]}\PY{p}{,} 
          \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Time}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{p}{[}\PY{l+m+mi}{150}\PY{p}{,}\PY{l+m+mi}{300}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{,} \PY{l+m+mi}{2000}\PY{p}{,} \PY{l+m+mi}{2500}\PY{p}{]}\PY{p}{\PYZcb{}}\PY{p}{)}
         
         \PY{n}{df\PYZus{}balanced}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}28}]:}         Différents models sur un subdataset balanced  F1-score val  Time
         0  Min - max - freq - energy on pulse et accelero{\ldots}      0.563882   150
         1                  Min - max - freq - energy on eegs      0.625435   300
         2                   Min - max - freq - energy on all      0.639735   400
         3         Min - max - freq - energy on all + Riemann      0.639785  2000
         4             Min - max - freq - energy on all + CSP      0.639209  2500
\end{Verbatim}
            
    Malgré de meilleurs résultats pour le F1-score pour le balanced dataset
(ATTENTION c'est bien le même validation set pour les 2 expériences !),
nous obtenons de biens moins bons scores sur le Kaggle (environ 59\%)
contre 64.22\% pour le all dataset avec la meilleure configuration. On
en déduit que le test set doit être très proche du tran set

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Sélection des features (caractéristiques)
\end{enumerate}

    \section{3. Modèle de classification : évaluation, optimisation et
choix}\label{moduxe8le-de-classification-uxe9valuation-optimisation-et-choix}

\subsection{3.1. Principe des modèles de classification
étudiés}\label{principe-des-moduxe8les-de-classification-uxe9tudiuxe9s}

Voici une explication théorique et aussi pédagogique que possible des
différents modèles considérés.

    \subsubsection{\texorpdfstring{3.1.1. \emph{Random
Forest}}{3.1.1. Random Forest}}\label{random-forest}

Il s'agit d'un algorithme utilisant une technique ensembliste adaptée à
la classification multi-classes et proposé en 2001 par Breiman \&
Cutter.

\paragraph{Principe}\label{principe}

L'élément de base est l'\textbf{arbre décisionnel ou arbre CART}
(Classi􏰂cation And Regression Tree) qui permet de décomposer le problème
de classification en une suite de tests correspondant à une partition de
l'espace des données en sous-régions homogènes en terme de classe.\\
Il est construit de manière \emph{récursive} à partir d'une base
d'apprentissage. Ces arbres fournissent non seulement une solution au
problème de classi􏰂cation mais permettent également de retracer les
questions à poser pour e􏰀ffectuer ce choix.

Les forêts sont en général plus efficaces que les simples arbres de
décision, dont la construction dépend de l'échantillon initial et qui
sont fortement instables, mais possèdent l'inconvénient d'être plus
di􏰃fficilement interprétables.\\
Une \textbf{forêt aléatoire} est un ensemble d'arbres de décision
binaires dans lequel a été introduit de l'aléatoire dans le but
d'obtenir des \emph{arbres les plus décorrélés possibles}. De cette
manière, on ne construit plus un seul arbre de décision mais plusieurs
arbres de décision, chacun ayant ses propres caractéristiques dues à
l'aléa introduit.\\
Le nouveau classifi􏰂eur correspondant à la forêt aléatoire est calculé
par \emph{vote majoritaire}, soit en prenant la majorité des votes de
chacun des classifi􏰂eurs. Ce n'est donc plus l'arbre qui vote, mais la
forêt (aléatoire) cachée derrière.\\
La forêt aléatoire constitue ainsi un modèle plus \emph{stable} que
l'arbre décisionnel.

􏰁\textbf{\emph{Random Forest}} est un type de forêt aléatoire qui
consiste pour à mélanger les arbres CART, le \emph{bagging}
(\emph{boostrap aggregating}) et le \emph{random subspace}.\\
Le principe simplifié de l'algorithme est décrit par le schéma
ci-dessous (fig 1.).\\
Pour chaque arbre, on tire un échantillon à partir de l'échantillon
initial. A chaque noeud, on choisit aléatoirement K variables et on
prend, parmi celles-ci, celle qui minimise le critère de l'algorithme
CART. On laisse grandir l'arbre jusqu'à ce qu'il n'y ait plus qu'un seul
élément dans chaque noeud.

\paragraph{\texorpdfstring{Deux approches: multi-classes ou
\emph{one-vs-all}}{Deux approches: multi-classes ou one-vs-all}}\label{deux-approches-multi-classes-ou-one-vs-all}

Les deux approches sont illustrées ci-dessous (fig.3).

L'approche plus classique \textbf{multi-classes} a d'abord été testée.
Le même algorithme est capable de distinguer les labels \{0,1,2,3,4\}.

Puis, l'approche \textbf{\emph{one-vs-all}} a été explorée dans laquelle
on construit successivement plusieurs modèles de classification
binaires. On attribue donc les labels classe par classe. Dans notre
problème, l'ordre des labels à attribuer est le suivant : 1.
différencier parmi les données les labels \{2, ≠2\} (classifieur \#1)
puis retirer les labels égaux à 2 2. différencier parmi le reste les
labels \{4, ≠4\} (classifieur \#2) puis retirer les labels égaux à 4 3.
différencier parmi le reste les labels \{3, ≠3\} (classifieur \#3) puis
retirer les labels égaux à 3 4. différencier parmi le reste les labels
\{0, 1(≠0)\} (classifieur \#4) puis conclure

Notons qu'aucun classifieur ne sera parfait. Ainsi, pour l'entraînement,
on retire à chaque étape les données correspondant \emph{réellement aux
labels i} afin que pour l'étape suivante la base d'apprentissage ne
contienne aucun de ces labels i. Mais, pour le test, on retirera les
données correspondant aux \emph{labels prédits i} par le classifieur. Le
reste contiendra donc certaines erreurs (faux-positifs prédits i mais
non égaux à i et faux-négatifs prédits comme non i mais égaux à i).

    \subsubsection{\texorpdfstring{3.1.2. \emph{Gradient Boosting Decision
Tree}}{3.1.2. Gradient Boosting Decision Tree}}\label{gradient-boosting-decision-tree}

\paragraph{Algorithme classique}\label{algorithme-classique}

Le \textbf{classifieur \emph{Gradient Boosting}} combine les éléments
suivants : \emph{boosting} (renforcement), descente de gradient et arbre
décisionnel.

Le \textbf{\emph{boosting}} pour la classification en Machine Learning
consiste à construire un ensemble de classifieurs de façon
\textbf{incrémentale}. A chaque étape, nous ajoutons un nouveau
\textbf{sous-modèle} simple de petit \textbf{arbre décisionnel}
(principe expliqué plus haut) pour éviter l'\emph{overfitting}, qui
essaie de \textbf{compenser les erreurs} faites par les sous-modèles
précédents.\\
Ce principe est illustré schématiquement plus bas (fig.4). L'algorithme
de \emph{gradient boosting} classique fait pousser les arbres
\textbf{horizontalement}, c'est-à-dire qu'il fait croître le niveau de
l'arbre.

Cette procédure de \emph{boosting} peut être considérée comme une forme
de descente de gradient.\\
En effet, rappelons que la \textbf{descente en gradient} optimise une
fonction de perte en appliquant la règle de mise à jour suivante à
plusieurs reprises :\\

\begin{equation*}
x = x = x - η∇Loss(x)
\end{equation*}

où ∇Loss(x) est le gradient de la fonction de perte et η est le taux
d'apprentissage

L'observation clé à retenir ici est que le gradient (négatif) de la
fonction de perte d'erreur au carré, évalué à la valeur de sortie
\(\hat{y}\), est le même que le résiduel (multiplié par 2) :\\

\begin{equation*}
Loss(yi,\hat{y}) = (yi - \hat{y})^2  \\
-∇Loss(\hat{y}) = 2*(yi - \hat{y})  
\end{equation*}

Or, dans l'algorithme de \emph{boosting}, on ajoute à chaque itération
un sous-modèle \textbf{qui essaie d'imiter le négatif de cette perte},
d'où le lien avec la descente de gradient. Le taux d'apprentissage η
serait de 1 dans ce cas.

\paragraph{\texorpdfstring{\emph{Light
GBM}}{Light GBM}}\label{light-gbm}

\textbf{Light GBM} est un gradient boosting framework qui utilise un
algorithme d'apprentissage basé sur l'arbre décisionnel. Lien vers la
documentation : https://lightgbm.readthedocs.io/en/latest/index.html.\\
Light GBM fait pousser l'arbre \textbf{verticalement}, soit les feuilles
de l'arbre, contrairement à l'agorithme classique de \emph{gradient
boosting} vu précédemment. Le principe est illustré ci-dessous (fig.5).
Il choisira la feuille avec une perte ∇ maximale pour grandir. Lorsqu'on
cultive la même feuille, l'algorithme de Leaf-wise peut réduire plus de
pertes qu'un algorithme de niveau.

Light GBM peut gérer la grande taille des données et nécessite moins de
mémoire pour fonctionner que le Gradient Boosting classique. Le préfixe
'Light' fait référence à la vitesse d'apprentissage plus élevée de
l'algorithme. Cet algorithme est recommandé pour des grands datasets
(\textgreater{} 10000 éléments) uniquement, ce qui est notre cas, car il
est sensible à l'\emph{overfitting} sinon.

    \subsubsection{\texorpdfstring{3.1.4. Comparaison \emph{a priori} des
modèles}{3.1.4. Comparaison a priori des modèles}}\label{comparaison-a-priori-des-moduxe8les}

\begin{longtable}[]{@{}llll@{}}
\toprule
\begin{minipage}[b]{0.32\columnwidth}\raggedright\strut
Algorithme\strut
\end{minipage} & \begin{minipage}[b]{0.17\columnwidth}\raggedright\strut
Avantages\strut
\end{minipage} & \begin{minipage}[b]{0.20\columnwidth}\raggedright\strut
Inconvénients\strut
\end{minipage} & \begin{minipage}[b]{0.20\columnwidth}\raggedright\strut
Usage habituel\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Random Forest\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
rapidité Robuste aux datasets bruités\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright\strut
Biaisé si classes non-équilibrées envers la classe la plus
représentée\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright\strut
Polyvalent\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Gradient Boosting\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
Bonne précision (rapidité et adapté aux grands datasets avec Light
GBM)\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright\strut
Peut \emph{overfitter} facilement Réglage des paramètres important\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright\strut
Polyvalent\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

    \subsection{3.2. Protocole de validation croisée pour l'évaluation de la
performance}\label{protocole-de-validation-croisuxe9e-pour-luxe9valuation-de-la-performance}

    Afin d'estimer la performance du modèle entraîné, nous devons choisir un
protocole de validation croisé parmi (entre autres): -
\textbf{leave-one-out (LOO)}\\
On entraîne N modèles sur N-1 échantillons et on les teste
respectivement sur l'échantillon retiré. - \textbf{K-fold}\\
Son principe est illustré ci-dessous pour K=5 (fig.7). On entraine K
modèles (K

    \subsection{3.3. Réglage des
hyperparamètres}\label{ruxe9glage-des-hyperparamuxe8tres}

    Pour trouver le meilleur lot de paramètres, on réalise souvent une
\textbf{recherche exhaustive par grille} d'hyperparamètres. Ici, nous
avons opté pour une \textbf{recherche aléatoire}. Les deux démarches
sont décrites ci-dessous (fig.8). La recherche aléatoire et la recherche
par grille explorent exactement \textbf{le même espace de paramètres}.
Le résultat des réglages de paramètres est assez similaire, tandis que
le \textbf{temps d'exécution} de la recherche aléatoire est nettement
inférieur.\\
Pour s'assurer que les grilles de recherche sont pertinentes, on
veillera à ce que le meilleur paramètre trouvé dans la grille ne soit
pas un paramètre extrême de la grille.

Pour chaque modèle, on emploira la démarche suivante : - sélectionner
quelques paramètres à ajuster et éventuellement un ordre de priorité -
ne garder que 20\% du train par soucis de rapidité et les mêmes 20\% par
soucis de reproductibilité - recherche aléatoire des meilleurs
hyperparamètres avec une crossvalidation K-fold avec K=3 (et non K=10,
par soucis de rapidité) - pour les meilleurs paramètres, estimer plus
finement les performances avec une crossvalidation K-fold avec K=10 sur
80\% du train - enfin, entraîner le modèle avec ces mêmes paramètres sur
tout le train

    \subsubsection{3.3.1. Random Forest}\label{random-forest}

Les paramètres par défaut du modèle implémenté sur la bibliothèque
scikit-learn sont :

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{RandomForestClassifier}
         \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
         \PY{n}{rf} \PY{o}{=} \PY{n}{RandomForestClassifier}\PY{p}{(}\PY{p}{)}
         \PY{n}{rf\PYZus{}default\PYZus{}params} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{rf}\PY{o}{.}\PY{n}{get\PYZus{}params}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RandomForest}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{rf\PYZus{}default\PYZus{}params}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}24}]:}               bootstrap class\_weight criterion max\_depth max\_features  \textbackslash{}
         RandomForest       True         None      gini      None         auto   
         
                      max\_leaf\_nodes  min\_impurity\_decrease min\_impurity\_split  \textbackslash{}
         RandomForest           None                    0.0               None   
         
                       min\_samples\_leaf  min\_samples\_split  min\_weight\_fraction\_leaf  \textbackslash{}
         RandomForest                 1                  2                       0.0   
         
                      n\_estimators n\_jobs  oob\_score random\_state  verbose  warm\_start  
         RandomForest         warn   None      False         None        0       False  
\end{Verbatim}
            
    \emph{n\_estimators} (nombre d'arbres dans la forêt ) et
\emph{max\_features} (taille des sous-ensembles aléatoires de
caractéristiques à prendre en compte lors du fractionnement d'un nœud)
sont les principaux paramètres à ajuster. On veut donc les considérer en
priorité. Mais, en raison du compromis temps/qualité posé par
\emph{n\_estimators}, il s'agira du dernier paramètre à estimer. Les
autres paramètres qui influencent l'apprentissage sont :
\emph{max\_depth}, \emph{min\_sample\_leaf}, \emph{min\_sample\_split}
et \emph{class\_weight}.

En pratique, on ajustera les paramètres ainsi : 1.
\textbf{\emph{max\_features}} : {[}'auto', 0.9, 0.95, 0.98, 0.99, 0.995,
0.998, 0.999, None{]}\\
Les bonnes valeurs empiriques par défaut sont
\emph{max\_features=sqrt(n\_features)} pour les tâches de
classification.\\
Méthode : \emph{GridSearchCV} sur la grille avec paramètres par défaut
pour le reste, validation croisée K-fold avec K=5 sur 20\% du train.\\
Résultat : \textbf{max\_features = 0.998} 2. \textbf{\emph{max\_depth}}
: {[}10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None{]},
\textbf{\emph{min\_samples\_leaf}} : {[}1, 2, 4{]} et
\textbf{\emph{min\_samples\_split}} : {[}2, 5, 10{]}\\
De bons résultats sont souvent obtenus en réglant max\_depth=None en
combinaison avec min\_samples\_split=2 (c'est-à-dire lorsque les arbres
sont en plein développement).\\
Méthode : \emph{RandomizeGridSearchCV} sur une grille de 100
combinaisons possibles mais 50 combinaisons aléatoirement choisies
testées, avec le meilleur \emph{max\_features} et les paramètres par
défaut pour le reste, validation croisée K-fold avec K=3 sur 20\% du
train.\\
Résultats : \textbf{max\_depth = None, min\_samples\_leaf = 2,
min\_samples\_split = 2} 3. \textbf{\emph{boostrap}} : {[}True,
False{]}, \textbf{\emph{class\_weight}} : {[}None, 'balanced'{]}\\
Les échantillons bootstrap sont utilisés par défaut (bootstrap=True)
alors que la stratégie par défaut pour les arbres supplémentaires est
d'utiliser l'ensemble des données (bootstrap=False).\\
Méthode : pour chacune des quatre possibilités, validation croisée
K-fold avec K=10 sur 20\% du train avec les meilleurs paramètres trouvés
précedemment et les paramètres par défaut pour le reste.\\
Résultat : \textbf{class\_weight = None}, \textbf{boostrap = True} 4.
\textbf{\emph{n\_estimators}} : {[}750, 1000, 1250, 1500, 1750, 2000,
2250, 2500, 2750, 3000{]}\\
Plus il est grand, mieux c'est, mais aussi plus il faudra de temps pour
le calculer. De plus, les résultats cesseront de s'améliorer de façon
significative au-delà d'un nombre critique d'arbres.\\
Méthode : \emph{GridSearchCV} sur la grille avec les meilleurs
paramètres trouvés précedemment et les paramètres par défaut pour le
reste, validation croisée K-fold avec K=3 sur 20\% du train.\\
Résultat : \textbf{n\_estimators = 1500}

\begin{longtable}[]{@{}lll@{}}
\toprule
\begin{minipage}[b]{0.41\columnwidth}\raggedright\strut
Modèle\strut
\end{minipage} & \begin{minipage}[b]{0.26\columnwidth}\raggedright\strut
Moyenne F1-score sur 20\% du train\strut
\end{minipage} & \begin{minipage}[b]{0.24\columnwidth}\raggedright\strut
Écart-type F1-score sur 20\% du train\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
Random Forest, paramètres par défaut\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\raggedright\strut
0.5595\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\raggedright\strut
0.78\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
Random Forest, paramètres optimisés\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\raggedright\strut
0.5711\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\raggedright\strut
0.93\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}79}]:} \PY{c+c1}{\PYZsh{} load all features for train and test and train labels}
         \PY{k+kn}{import} \PY{n+nn}{csv}
         \PY{n}{label\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dataset/data/train\PYZus{}label.npy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{features\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dataset/data/all\PYZus{}features\PYZus{}train.npy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{features\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dataset/data/all\PYZus{}features\PYZus{}test.npy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{shape all dataset features\PYZus{}train: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{features\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{, features\PYZus{}test: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{features\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} final estimation on 80\PYZpc{} of the whole dataset only}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
         \PY{n}{features}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{label}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{features\PYZus{}train}\PY{p}{,} \PY{n}{label\PYZus{}train}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{shape 20}\PY{l+s+si}{\PYZpc{} d}\PY{l+s+s2}{ataset features: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{features}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{, X\PYZus{}test: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{label}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
shape all dataset features\_train:  (38289, 1366) , features\_test:  (37439, 1366)
shape 20\% dataset features:  (30631, 1366) , X\_test:  30631

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}91}]:} \PY{c+c1}{\PYZsh{} random forest hyperparameter tuning}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{cross\PYZus{}val\PYZus{}score}
         \PY{n}{best\PYZus{}RF\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+m+mf}{0.998}\PY{p}{,}
                           \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}split}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{2}\PY{p}{,}
                           \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{2}\PY{p}{,}
                           \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{k+kc}{None}\PY{p}{,}
                           \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{class\PYZus{}weight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{k+kc}{None}\PY{p}{,}
                           \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bootstrap}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{k+kc}{True}\PY{p}{,}
                           \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1500}\PY{p}{\PYZcb{}}
         \PY{n}{rf} \PY{o}{=} \PY{n}{RandomForestClassifier}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{best\PYZus{}RF\PYZus{}params}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} rf\PYZus{}cv\PYZus{}score = cross\PYZus{}val\PYZus{}score(rf, features, label, cv=10, verbose=10) \PYZsh{} final evaluation K\PYZhy{}foldCV (K=10) on 80\PYZpc{} of the whole dataset}
         \PY{c+c1}{\PYZsh{} rf.fit(features\PYZus{}train, label\PYZus{}train) \PYZsh{} train the model with best parameters on the whole dataset}
         \PY{c+c1}{\PYZsh{} rf.predict(features\PYZus{}test) \PYZsh{} apply on test set}
\end{Verbatim}


    \subsubsection{3.3.2. Gradient Boosting}\label{gradient-boosting}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{GradientBoostingClassifier}
         \PY{n}{gb} \PY{o}{=} \PY{n}{GradientBoostingClassifier}\PY{p}{(}\PY{p}{)}
         \PY{n}{gb\PYZus{}default\PYZus{}params} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{gb}\PY{o}{.}\PY{n}{get\PYZus{}params}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{GradientBoosting}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{gb\PYZus{}default\PYZus{}params}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}25}]:}                      criterion  init  learning\_rate      loss  max\_depth  \textbackslash{}
         GradientBoosting  friedman\_mse  None            0.1  deviance          3   
         
                          max\_features max\_leaf\_nodes  min\_impurity\_decrease  \textbackslash{}
         GradientBoosting         None           None                    0.0   
         
                          min\_impurity\_split  min\_samples\_leaf     {\ldots}      \textbackslash{}
         GradientBoosting               None                 1     {\ldots}       
         
                           min\_weight\_fraction\_leaf  n\_estimators  n\_iter\_no\_change  \textbackslash{}
         GradientBoosting                       0.0           100              None   
         
                          presort random\_state subsample     tol  validation\_fraction  \textbackslash{}
         GradientBoosting    auto         None       1.0  0.0001                  0.1   
         
                           verbose  warm\_start  
         GradientBoosting        0       False  
         
         [1 rows x 21 columns]
\end{Verbatim}
            
    On ajustera tous les paramètres qui influencent le résultat en même
temps à l'exception du \emph{n\_estimators}, en raison du compromis
temps/qualité qu'il pose. De plus, on ne peut ajuster en même temps le
\emph{learning\_rate} et \emph{n\_estimators} car il est nécessaire de
trouver un compromis entre ces deux paramètres.

En pratique, on ajustera les paramètres ainsi : 1.
\textbf{\emph{max\_depth}} : {[}10, 20, 30, 40, 50, 60, 70, 80, 90, 100,
110, None{]}, \textbf{\emph{min\_sample\_leaf}} : {[}1, 2, 4{]} et
\textbf{\emph{min\_sample\_split}} : {[}2, 5, 10{]},
\textbf{\emph{subsample}} : {[}0.8, 1.0{]},
\textbf{\emph{learning\_rate}} : {[}1, 0.1, 0.01, 0.001, 0.0001{]},
\textbf{\emph{max\_features}} : {[}'auto', 'log2', 0.95, 0.98, 0.99,
0.995, None{]}\\
Méthode : \emph{RandomizeGridSearchCV} sur une grille de 100
combinaisons possibles mais 20 combinaisons aléatoirement choisies
testées, avec les paramètres par défaut pour le reste, validation
croisée K-fold avec K=3 sur 20\% du .\\
Résultats : \textbf{max\_depth = 100, min\_sample\_leaf = 4,
min\_sample\_split = 5, subsample = 0.8, learning\_rate = 0.1,
max\_features = 0.99.} 2. \textbf{\emph{n\_estimators}} : {[}500, 700,
1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750{]}\\
Méthode : \emph{GridSearchCV} sur la grille avec les meilleurs
paramètres trouvés précedemment et les paramètres par défaut pour le
reste, validation croisée K-fold avec K=3 sur 20\% du train.\\
Résultat : \textbf{n\_estimators = 1500}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}92}]:} \PY{c+c1}{\PYZsh{} gradient boosting hyperparameter tuning}
         \PY{n}{best\PYZus{}GB\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subsample}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mf}{0.8}\PY{p}{,}
                           \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}split}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{5}\PY{p}{,}
                           \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{4}\PY{p}{,}
                           \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mf}{0.99}\PY{p}{,}
                           \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{100}\PY{p}{,}
                           \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{learning\PYZus{}rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mf}{0.1}\PY{p}{,}
                           \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1500}
                          \PY{p}{\PYZcb{}}
         \PY{n}{gb} \PY{o}{=} \PY{n}{GradientBoostingClassifier}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{best\PYZus{}GB\PYZus{}params}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} gb\PYZus{}cv\PYZus{}score = cross\PYZus{}val\PYZus{}score(gb, features, label, cv=10, verbose=10) \PYZsh{} final evaluation K\PYZhy{}foldCV (K=10) on 80\PYZpc{} of the whole dataset}
         \PY{c+c1}{\PYZsh{} gb.fit(features\PYZus{}train, label\PYZus{}train) \PYZsh{} train the model with best parameters on the whole dataset}
         \PY{c+c1}{\PYZsh{} y\PYZus{}pred\PYZus{}gb = gb.predict(features\PYZus{}test) \PYZsh{} apply on test set}
\end{Verbatim}


    \subsection{3.4. Autres tentatives}\label{autres-tentatives}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{}\PYZsh{} One\PYZhy{}vs\PYZhy{}all}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} Light gbm }
        \PY{k+kn}{import} \PY{n+nn}{lightgbm} \PY{k}{as} \PY{n+nn}{lgb}
        \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{features\PYZus{}train}\PY{p}{,} \PY{n}{label\PYZus{}train}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.33}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}
        \PY{n}{model} \PY{o}{=} \PY{n}{lgb}\PY{o}{.}\PY{n}{LGBMClassifier}\PY{p}{(}\PY{n}{objective}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{binary}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{reg\PYZus{}lambda}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,}\PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{10000}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{eval\PYZus{}set}\PY{o}{=}\PY{p}{[}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{]}\PY{p}{,} \PY{n}{early\PYZus{}stopping\PYZus{}rounds}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{)}
        \PY{n}{y\PYZus{}pred\PYZus{}lgbm} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{features\PYZus{}test}\PY{p}{)}
\end{Verbatim}


    \section{4. Comparaison, choix des modèles \& synthèse des
résultats}\label{comparaison-choix-des-moduxe8les-synthuxe8se-des-ruxe9sultats}

    Pour évaluer l'augmentation de la performance entre les paramètres par
défaut et ces paramètres ajustés, on regarde l'évolution du F1-score
entre avec une validation croisée K-fold, K=10 sur 80\% du training set.

\begin{longtable}[]{@{}lll@{}}
\toprule
\begin{minipage}[b]{0.32\columnwidth}\raggedright\strut
Modèle et paramètres\strut
\end{minipage} & \begin{minipage}[b]{0.26\columnwidth}\raggedright\strut
Résultat de F1-score moyen sur 80\% du dataset en CV\strut
\end{minipage} & \begin{minipage}[b]{0.32\columnwidth}\raggedright\strut
Résultat en F1-score en soumission\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Random Forest, paramètres par défaut\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\raggedright\strut
0.61330\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
0.64223\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Random Forest, paramètres optimisés\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\raggedright\strut
0.62951\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
0.64303\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Random Forest OneVSall\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\raggedright\strut
\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Gradient Boosting, paramètres par défaut\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\raggedright\strut
\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
0.63320\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Gradient Boosting, paramètres optimisés\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\raggedright\strut
\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
Light GBM\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\raggedright\strut
0.67140\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright\strut
0.64554\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Ce tableau récapitulatif permet de constater que la richesse des
\emph{features} permet d'obtenir de bons f1-scores même avec les
paramètres par défaut des différents modèles. L'apport du réglage des
hyperparmètres sur les résultats est relativement faible.\\
Les modèles les plus intéressants à conserver paraissent être le Light
GBM qui donne les meilleurs résultats en soumission (soit sur 50\% du
test) ainsi que le Random Forest avec les paramètres optimisés.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
